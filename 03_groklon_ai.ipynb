{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZObWRXrapR4-",
        "outputId": "93f07ae3-f314-4b2f-9ace-a48fd270c94e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.4.26)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.2)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.9.3)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.4.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ratelimit\n",
        "!pip install trafilatura\n",
        "import trafilatura\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field\n",
        "import google.generativeai as genai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python Script for AI-Powered Insight Extraction from Scientific Papers: A Step-by-Step Explanation\n",
        "\n",
        "This script is the third stage in a data processing pipeline. It takes the previously fetched full-text content of scientific papers and uses Google's Gemini AI model to analyze each paper in the context of specific genes and variants identified in an input CSV file (likely generated by the first script in your series).\n",
        "\n",
        "**Overall Goal:**\n",
        "\n",
        "The primary objective is to:\n",
        "1.  Read a list of PMIDs, associated genes, variants, and other data from an input CSV file.\n",
        "2.  For each row in the CSV, retrieve the corresponding paper's full-text content (HTML/XML) from a saved dictionary (likely generated by the second script).\n",
        "3.  Extract clean, readable text from the raw HTML or XML content.\n",
        "4.  Construct a highly specific prompt for the Gemini AI model. This prompt will include the extracted paper text AND the particular gene/variant from the current CSV row that needs analysis.\n",
        "5.  Send this prompt to the Gemini API and request structured JSON output containing specific fields like \"findings,\" \"p_value,\" \"population,\" \"variant_details,\" and \"gene_association\" relevant *only* to the targeted gene/variant in that row.\n",
        "6.  Parse the AI's response, validate it, and augment the original CSV row data with these newly extracted insights.\n",
        "7.  Save the fully augmented dataset to new JSON and CSV files.\n",
        "\n",
        "---\n",
        "\n",
        "## The Pipeline: Step-by-Step Explanation\n",
        "\n",
        "### Phase 1: Setup, Configuration, and API Initialization\n",
        "\n",
        "This section is crucial for setting up the environment, including necessary libraries, logging, and configuring the Gemini API.\n",
        "\n",
        "* **Imports (Lines 1-15):**\n",
        "    * Standard libraries: `csv`, `json`, `time`, `logging`, `pickle`, `gzip`, `glob`, `os`, `re`.\n",
        "    * Type hinting: `Optional`, `Dict`, `Any`, `List`.\n",
        "    * `tqdm`: For progress bars.\n",
        "    * `pandas`: For handling CSV data.\n",
        "    * `BeautifulSoup`: For parsing HTML/XML (used as a fallback for text extraction).\n",
        "    * `ratelimit`: A library to control the rate of API calls, preventing quota overruns.\n",
        "    * `pydantic`: For data validation and defining data structures (like `ExtractedData`).\n",
        "    * `google.generativeai` (as `genai`): The official Google library for interacting with the Gemini API.\n",
        "    * `concurrent.futures`: For parallel processing of API calls to speed things up.\n",
        "    * `trafilatura`: A robust library for extracting main text content from web pages.\n",
        "\n",
        "* **API Key and Logging Configuration (Lines 17-30):**\n",
        "    * `GEMINI_API_KEY_HARDCODED`: **A CRITICAL SECURITY WARNING is embedded in the script itself and highlighted here: Hardcoding API keys is a major security risk.** The script uses a placeholder key and logs a critical warning if it's not changed.\n",
        "    * `logging.basicConfig(...)`: Sets up logging to a file (`gemini_processing_errors.log`) to record detailed information about the script's execution, including errors and warnings.\n",
        "    * `logger = logging.getLogger(__name__)`: Creates a logger instance.\n",
        "\n",
        "* **Gemini Model Initialization (Lines 32-57):**\n",
        "    * Checks if the `GEMINI_API_KEY_HARDCODED` is a placeholder. If so, it logs and prints a critical warning.\n",
        "    * `genai.configure(api_key=...)`: Configures the Gemini library with the API key.\n",
        "    * `gemini_config_settings`: A dictionary to hold various settings for the Gemini API calls:\n",
        "        * `model_name`: Specifies which Gemini model to use (e.g., 'gemini-1.5-flash-latest').\n",
        "        * `temperature`: Controls the creativity of the AI's responses (lower is more deterministic).\n",
        "        * `max_output_tokens`: Maximum length of the AI's response.\n",
        "        * `request_timeout`: How long to wait for an API response.\n",
        "        * `calls_per_period`, `period_seconds`: Parameters for rate limiting (e.g., 15 calls per 60 seconds).\n",
        "        * `max_gemini_workers`: Number of parallel calls to make to Gemini.\n",
        "        * `max_content_chars`: Maximum number of characters from the paper text to send in a single prompt.\n",
        "    * `model = genai.GenerativeModel(...)`: Initializes an instance of the generative model.\n",
        "    * Includes error handling in case API configuration or model initialization fails.\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 2: Data Models\n",
        "\n",
        "This defines the expected structure of the data extracted by Gemini.\n",
        "\n",
        "* **`ExtractedData(BaseModel)` class (Lines 59-64):**\n",
        "    * Uses `Pydantic` to define a data model. This ensures that the data received from Gemini (after JSON parsing) conforms to an expected structure and types.\n",
        "    * Fields include `findings`, `p_value`, `population`, `variant_details`, and `gene_association`. Each is optional and defaults to `None`.\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 3: Helper Functions\n",
        "\n",
        "These functions perform specific, often-reused tasks within the script.\n",
        "\n",
        "1.  **`call_gemini_api_with_retries(...)` function (Lines 67-131):**\n",
        "    * **Purpose:** A robust wrapper for making calls to the Gemini API, incorporating rate limiting and retry logic.\n",
        "    * **Decorators:**\n",
        "        * `@sleep_and_retry`: If a `RateLimitException` occurs (from the `@limits` decorator), this will pause execution and retry.\n",
        "        * `@limits(...)`: Enforces the rate limits defined in `gemini_config_settings` (e.g., 15 calls per 60 seconds).\n",
        "    * **How it works:**\n",
        "        * Checks if the `current_model_instance` (Gemini model) is initialized.\n",
        "        * Tries to make an API call using `current_model_instance.generate_content()`.\n",
        "        * Specifies `response_mime_type='application/json'` to request JSON output from Gemini.\n",
        "        * Checks for prompt blocking by Gemini (safety filters).\n",
        "        * Checks if the response contains candidates and if the generation finished successfully.\n",
        "        * **Error Handling & Retries:** If an exception occurs (e.g., API errors, rate limit errors indicated by \"429\"), it implements an exponential backoff strategy (waits for an increasing `base_delay` multiplied by 2 to the power of the attempt number) before retrying, up to `max_retries`. For 429 errors, it tries to respect the \"Retry-After\" header if provided by the API.\n",
        "    * **Output:** Returns the Gemini API `response` object or `None` if all attempts fail or the prompt is blocked.\n",
        "\n",
        "2.  **`clean_json_response(text: str)` function (Lines 133-139):**\n",
        "    * **Purpose:** To clean up potential minor formatting issues in the text string received from Gemini before attempting to parse it as JSON.\n",
        "    * **How it works:** Removes common artifacts like markdown code fences (e.g., \"\\`\\`\\`json\" and \"\\`\\`\\`\") or the word \"json\" if it prefixes the actual JSON object.\n",
        "\n",
        "3.  **`extract_text_from_html(current_logger, html_content, pmid)` function (Lines 141-193):**\n",
        "    * **Purpose:** To extract clean, readable text from raw HTML content.\n",
        "    * **How it works (Primary method - `trafilatura`):**\n",
        "        * First, it tries to use the `trafilatura.extract()` function. Trafilatura is a specialized library designed to extract the main textual content from web pages, often yielding high-quality results by removing boilerplate like ads, menus, and footers.\n",
        "    * **How it works (Fallback method - `BeautifulSoup`):**\n",
        "        * If `trafilatura` fails or extracts insufficient text, it falls back to using `BeautifulSoup`.\n",
        "        * It manually removes common HTML tags that don't contain main content (`script`, `style`, `header`, `footer`, `nav`, etc.).\n",
        "        * It also removes elements matching a list of `noisy_selectors` (CSS selectors for common sidebars, comment sections, share buttons, etc.).\n",
        "        * Then, it tries to find common main content selectors (`article`, `main`, `.article-body`, etc.). If found, it extracts text from that area.\n",
        "        * As a last resort within BeautifulSoup, it extracts all `stripped_strings` from the `body` or the root of the document.\n",
        "    * **Output:** Returns the extracted plain text string, or an empty string if extraction fails.\n",
        "\n",
        "4.  **`extract_text_from_xml(current_logger, xml_content, pmid)` function (Lines 195-238):**\n",
        "    * **Purpose:** To extract clean text from XML content (often JATS XML from PMC).\n",
        "    * **How it works (using `BeautifulSoup` with 'xml' parser):**\n",
        "        * Removes a list of common XML tags that usually don't contain the main narrative content (`table-wrap`, `fig`, `contrib-group`, `ref-list`, `article-meta`, etc.).\n",
        "        * Prioritizes extracting text from the `<body>` tag if present.\n",
        "        * If no substantial body text, it tries to combine text from `<article-title>` and `<abstract>` tags.\n",
        "        * As a final fallback, it extracts all `stripped_strings` from the entire XML document.\n",
        "    * **Output:** Returns the extracted plain text string, or an empty string.\n",
        "\n",
        "5.  **`get_text_from_content_item(current_logger, pmid, content_item)` function (Lines 240-260):**\n",
        "    * **Purpose:** A dispatcher function that calls the appropriate text extraction function (`extract_text_from_html` or `extract_text_from_xml`) based on the `type` specified in the `content_item` (which comes from the `content_dict.pkl.gz` file generated by the previous script).\n",
        "    * Handles cases where content might be missing or marked as an error.\n",
        "    * If the type is unknown, it attempts HTML extraction first, then XML as a fallback.\n",
        "\n",
        "6.  **`process_single_csv_row_with_gemini(...)` function (Lines 263-359):**\n",
        "    * **Purpose:** This is the core logic for processing *one individual row* from the input CSV file. For each row, it targets the specific gene/variant combination in that row for AI analysis using the full text of the associated paper.\n",
        "    * **How it works:**\n",
        "        1.  **Initialization:** Takes the logger, Gemini model instance, PMID, the original CSV row data (as a dictionary), the `content_dict` (containing full texts), a `content_cache` (to store extracted plain text for PMIDs to avoid re-extraction if a PMID appears in multiple rows), and Gemini config.\n",
        "        2.  **Get Paper Text:**\n",
        "            * Retrieves the raw HTML/XML content for the `pmid` from `content_dict`.\n",
        "            * If the plain text for this `pmid` isn't already in `content_cache`, it calls `get_text_from_content_item` to extract it and then stores it in the cache.\n",
        "        3.  **Handle Insufficient Text:** If the extracted text is too short (or if the title is also too short as a last resort), it logs a warning and skips Gemini processing for this row.\n",
        "        4.  **Truncate Text:** If the extracted text is longer than `max_content_chars` allowed by the Gemini configuration, it truncates the text.\n",
        "        5.  **Construct Targeted Prompt:**\n",
        "            * This is a key step. It builds a detailed prompt for Gemini.\n",
        "            * The prompt includes:\n",
        "                * The PMID and Title of the paper.\n",
        "                * **Crucially, it specifies the exact Gene, Variant, and Alleles from the *current CSV row* that Gemini should focus its analysis on.** This ensures that the AI's response is targeted to the specific item of interest for that row.\n",
        "                * The (potentially truncated) full text of the paper.\n",
        "                * Instructions for Gemini to output a single JSON object with predefined keys: `\"findings\"`, `\"p_value\"`, `\"population\"`, `\"variant_details\"`, `\"gene_association\"`.\n",
        "        6.  **Call Gemini API:** Invokes `call_gemini_api_with_retries` with the constructed prompt.\n",
        "        7.  **Process Response:**\n",
        "            * If Gemini returns a valid text response:\n",
        "                * Cleans the response using `clean_json_response`.\n",
        "                * Parses the cleaned text as JSON. It expects a single JSON object.\n",
        "                * **Validates and Updates:**\n",
        "                    * It converts the `variant_details` field from Gemini to a string if it's a dictionary (sometimes AI might return structured data here).\n",
        "                    * Uses the `ExtractedData` Pydantic model to validate the structure and types of the parsed JSON data.\n",
        "                    * Updates the `augmented_row` (a copy of the original CSV row) with the values from the validated Gemini output.\n",
        "                    * Includes a fallback for `variant_details`: if Gemini returns `null` for it, the script uses the original 'Variant' value from the input CSV row.\n",
        "            * Handles `JSONDecodeError` if the response isn't valid JSON, and other general exceptions.\n",
        "        8.  **Output:** Returns the `augmented_row` dictionary, now containing the original CSV data plus the new fields populated by Gemini. If errors occur, it returns the row with Gemini fields as `None`.\n",
        "\n",
        "---\n",
        "\n",
        "### Phase 4: Main Script Logic (`main()` function)\n",
        "\n",
        "This function orchestrates the entire workflow.\n",
        "\n",
        "1.  **Initial Checks & Setup (Lines 362-370):**\n",
        "    * Ensures the global Gemini `model` is initialized.\n",
        "    * Sets up paths to find the input CSV (`pubmed_genetic_results_*.csv`) and the content dictionary (`content_dict.pkl.gz`). It searches in several standard locations (current directory, `data/output/`, `../data/output/`).\n",
        "\n",
        "2.  **Load Data (Lines 372-402):**\n",
        "    * Reads the latest `pubmed_genetic_results_*.csv` file into a pandas DataFrame. Renames 'pmid' to 'PMID' if necessary. Ensures 'PMID' column exists.\n",
        "    * Loads the `content_dict` from the `content_dict.pkl.gz` file. Keys (PMIDs) are converted to strings.\n",
        "\n",
        "3.  **Filter DataFrame (Lines 404-420):**\n",
        "    * Identifies `valid_pmids_with_content` from `content_dict` (PMIDs that have non-error, non-empty text content).\n",
        "    * Filters the input DataFrame (`df`) to keep only rows whose `PMID` is present in `valid_pmids_with_content`. This ensures that Gemini is only called for papers where usable text is available.\n",
        "    * Logs and prints how many rows were removed due to missing content.\n",
        "\n",
        "4.  **Testing Slice (Lines 422-430):**\n",
        "    * A feature for testing: If the filtered DataFrame `df_filtered` has more than `num_rows_to_test` (e.g., 120) rows, it processes only the first `num_rows_to_test`. This is useful for quick tests without running the AI on the entire dataset.\n",
        "\n",
        "5.  **Prepare and Execute Gemini Processing Tasks (Lines 432-473):**\n",
        "    * `max_gemini_workers`: From the config, determines how many parallel API calls can be made.\n",
        "    * `text_extraction_cache`: An empty dictionary to cache extracted plain text from `content_dict` items. If multiple CSV rows refer to the same PMID, the text extraction from its HTML/XML only happens once.\n",
        "    * **Task Creation:** It iterates through each row of the `df_to_process` (the filtered, possibly sliced DataFrame). For each row, it creates a tuple `task_args` containing all arguments needed by `process_single_csv_row_with_gemini`.\n",
        "    * **Parallel Execution (if `max_gemini_workers > 1`):**\n",
        "        * Uses `concurrent.futures.ThreadPoolExecutor` to process the tasks (each representing a CSV row) in parallel.\n",
        "        * `executor.submit(process_single_csv_row_with_gemini, *task_args)`: Submits each task to the thread pool.\n",
        "        * `tqdm` provides a progress bar for the parallel execution.\n",
        "        * As each task (`future`) completes, its result (the augmented row dictionary) is appended to `all_processed_results_final`.\n",
        "        * Includes error handling for exceptions that might occur within a thread.\n",
        "    * **Sequential Execution (if `max_gemini_workers <= 1`):**\n",
        "        * If not using multiple workers, it processes tasks one by one in a simple loop, again with a `tqdm` progress bar.\n",
        "\n",
        "6.  **Save Results (Lines 475-509):**\n",
        "    * Determines the output directory (tries `data/output/` or current directory).\n",
        "    * **JSON Output:** Saves the `all_processed_results_final` list (containing all augmented row dictionaries) to a timestamped JSON file (e.g., `gemini_extracted_row_by_row_20250531_205424.json`).\n",
        "    * **CSV Output:**\n",
        "        * If results exist, converts `all_processed_results_final` into a pandas DataFrame.\n",
        "        * It attempts to order the columns in the output CSV: first the original columns from the input CSV, then the new Gemini-added columns, then any other unexpected columns.\n",
        "        * Saves this final DataFrame to a timestamped CSV file.\n",
        "    * Logs and prints messages about where the files were saved.\n",
        "\n",
        "7.  **Script Execution Trigger (`if __name__ == \"__main__\":`) (Lines 513-514):**\n",
        "    * Ensures `main()` is called when the script is run directly.\n",
        "\n",
        "---\n",
        "\n",
        "This script represents a sophisticated approach to automated, targeted information extraction from scientific literature using generative AI. Key features include robust API interaction, intelligent text extraction, precise AI prompting for row-specific details, and parallel processing for efficiency. The emphasis on security (API key handling) and rate limiting is also crucial for real-world application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5JMDXLK30ABh"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any, List # Added List\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field\n",
        "import google.generativeai as genai\n",
        "import concurrent.futures # For parallelization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "OxfRSuWy4B3D",
        "outputId": "2ca8c7cd-a7a6-449b-cd9f-e954bfd308fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CRITICAL:__main__:Using a placeholder or example Gemini API key. Please update it in the script.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚨 CRITICAL WARNING: Using a placeholder or example Gemini API key. Please update it in the script.\n",
            "Gemini model 'models/gemini-1.5-flash-latest' initialized.\n",
            "Using input CSV file: /content/pubmed_genetic_results_68a3f3d2.csv\n",
            "Loaded 923 from content_dict.pkl.gz\n",
            "INFO: Filtered CSV from 4695 to 3499 rows. 1196 rows were removed because their PMIDs lacked valid, non-empty content in 'content_dict.pkl.gz'.\n",
            "INFO: TESTING MODE - Will process only the first 120 rows.\n",
            "Number of CSV rows to process: 120\n",
            "Starting Gemini processing for 120 CSV rows. Using 1 worker(s).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing CSV rows (Gemini Sequential):  48%|████▊     | 57/120 [03:24<02:08,  2.04s/row]ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 533.38ms\n",
            "Processing CSV rows (Gemini Sequential):  50%|█████     | 60/120 [03:31<02:10,  2.17s/row]WARNING:trafilatura.core:discarding data: None\n",
            "WARNING:__main__:BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID 18692774. Length: 0\n",
            "WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: Imd/nan.\n",
            "Processing CSV rows (Gemini Sequential):  51%|█████     | 61/120 [04:00<09:48,  9.98s/row]WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: PGRP-LC/nan.\n",
            "Processing CSV rows (Gemini Sequential):  52%|█████▏    | 62/120 [04:01<07:04,  7.33s/row]WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: commensal-gut/nan.\n",
            "Processing CSV rows (Gemini Sequential):  80%|████████  | 96/120 [06:11<01:23,  3.47s/row]WARNING:trafilatura.core:discarding data: None\n",
            "WARNING:__main__:BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID 16803893. Length: 0\n",
            "WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: AcPIM2/nan.\n",
            "Processing CSV rows (Gemini Sequential):  81%|████████  | 97/120 [06:12<01:04,  2.81s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: AcPIM4-6/nan.\n",
            "Processing CSV rows (Gemini Sequential):  82%|████████▏ | 98/120 [06:13<00:49,  2.25s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: Asp/nan.\n",
            "Processing CSV rows (Gemini Sequential):  82%|████████▎ | 99/120 [06:14<00:38,  1.84s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PIM/nan.\n",
            "Processing CSV rows (Gemini Sequential):  83%|████████▎ | 100/120 [06:15<00:32,  1.62s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PPM-dependent mannosyl-transferase/nan.\n",
            "Processing CSV rows (Gemini Sequential):  84%|████████▍ | 101/120 [06:16<00:28,  1.49s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PimE/nan.\n",
            "Processing CSV rows (Gemini Sequential): 100%|██████████| 120/120 [07:34<00:00,  3.79s/row]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved final 120 augmented rows to /content/gemini_extracted_row_by_row_20250531_174830.json\n",
            "Saved final 120 augmented rows to /content/gemini_extracted_row_by_row_20250531_174830.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "import google.generativeai as genai\n",
        "import concurrent.futures\n",
        "import trafilatura\n",
        "\n",
        "# --- Logging and API Configuration ---\n",
        "# WARNING: Hardcoding API keys is a significant security risk.\n",
        "GEMINI_API_KEY_HARDCODED = \"AIzaSyADS35cYhRoTNH6OCE2TpH4CMCJkeyTAMc\"  # Replace with your actual key\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='gemini_processing_errors.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "model = None\n",
        "gemini_config_settings = {}\n",
        "\n",
        "try:\n",
        "    # More robust check for placeholder/example key\n",
        "    is_placeholder_key = (\n",
        "        not GEMINI_API_KEY_HARDCODED or\n",
        "        GEMINI_API_KEY_HARDCODED.startswith(\"YOUR_\") or\n",
        "        GEMINI_API_KEY_HARDCODED == \"AIzaSyADS35cYhRoTNH6OCE2TpH4CMCJkeyTAMc\" # Example placeholder\n",
        "    )\n",
        "    if is_placeholder_key:\n",
        "        logger.critical(\"Using a placeholder or example Gemini API key. Please update it in the script.\")\n",
        "        print(\"🚨 CRITICAL WARNING: Using a placeholder or example Gemini API key. Please update it in the script.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY_HARDCODED)\n",
        "    logger.info(\"Gemini API configured with hardcoded key.\")\n",
        "\n",
        "    gemini_config_settings = {\n",
        "        'model_name': 'gemini-1.5-flash-latest',\n",
        "        'temperature': 0.1,\n",
        "        'max_output_tokens': 8192,\n",
        "        'request_timeout': 400,\n",
        "        'calls_per_period': 15,\n",
        "        'period_seconds': 60,\n",
        "        'max_gemini_workers': 1, # Default, can be changed for row-level parallelism\n",
        "        'max_content_chars': 150000\n",
        "    }\n",
        "    # If loading from a config file, update gemini_config_settings here.\n",
        "\n",
        "    model = genai.GenerativeModel(gemini_config_settings['model_name'])\n",
        "    logger.info(f\"Gemini model '{model.model_name}' initialized.\")\n",
        "    print(f\"Gemini model '{model.model_name}' initialized.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.critical(f\"Failed to configure Gemini API or initialize model: {e}\", exc_info=True)\n",
        "    print(f\"🚨 CRITICAL ERROR: Failed to configure Gemini API or initialize model. Error: {e}\")\n",
        "\n",
        "# --- Data Models ---\n",
        "class ExtractedData(BaseModel):\n",
        "    findings: Optional[str] = Field(default=None)\n",
        "    p_value: Optional[str] = Field(default=None)\n",
        "    population: Optional[str] = Field(default=None)\n",
        "    variant_details: Optional[str] = Field(default=None)\n",
        "    gene_association: Optional[str] = Field(default=None)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "@sleep_and_retry\n",
        "@limits(calls=gemini_config_settings.get('calls_per_period', 15),\n",
        "        period=gemini_config_settings.get('period_seconds', 60))\n",
        "def call_gemini_api_with_retries(\n",
        "    current_logger: logging.Logger,\n",
        "    current_model_instance: genai.GenerativeModel, # Renamed for clarity\n",
        "    prompt: str,\n",
        "    current_gemini_config: dict,\n",
        "    max_retries=2,\n",
        "    base_delay=10\n",
        "):\n",
        "    if not current_model_instance:\n",
        "        current_logger.error(\"Gemini model is not initialized. Cannot make API call.\")\n",
        "        return None\n",
        "\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            current_logger.debug(f\"Gemini API call attempt {attempt + 1} for prompt (first 100 chars): {prompt[:100]}\")\n",
        "            response = current_model_instance.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=current_gemini_config.get('temperature', 0.1),\n",
        "                    max_output_tokens=current_gemini_config.get('max_output_tokens', 8192),\n",
        "                    response_mime_type='application/json'\n",
        "                ),\n",
        "                request_options={'timeout': current_gemini_config.get('request_timeout', 400)}\n",
        "            )\n",
        "            current_logger.debug(f\"Gemini API response received. Safety: {response.prompt_feedback if response.prompt_feedback else 'N/A'}, Finish Reason: {response.candidates[0].finish_reason if response.candidates else 'N/A'}\")\n",
        "\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                block_msg = response.prompt_feedback.block_reason_message or \"No specific block message\"\n",
        "                current_logger.error(f\"Prompt blocked by Gemini API. Reason: {response.prompt_feedback.block_reason} - {block_msg}. Prompt: {prompt[:200]}\")\n",
        "                return None\n",
        "\n",
        "            if not response.candidates:\n",
        "                current_logger.warning(f\"No candidates in Gemini response for prompt: {prompt[:200]}\")\n",
        "                if attempt < max_retries:\n",
        "                    delay_val = base_delay * (2 ** attempt)\n",
        "                    current_logger.info(f\"Retrying due to no candidates in {delay_val}s...\")\n",
        "                    time.sleep(delay_val)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            if response.candidates[0].finish_reason != 1: # 1 is \"STOP\"\n",
        "                 current_logger.warning(f\"Gemini generation finished with reason: {response.candidates[0].finish_reason}. Content may be incomplete. Safety: {response.candidates[0].safety_ratings}\")\n",
        "\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            delay_val = base_delay * (2 ** attempt)\n",
        "            if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
        "                current_logger.warning(f\"Gemini API rate limit error (429) on attempt {attempt + 1}: {e}. Applying longer backoff.\")\n",
        "                try:\n",
        "                    error_response = getattr(e, 'response', None)\n",
        "                    if error_response and hasattr(error_response, 'headers'):\n",
        "                        retry_after_seconds = int(error_response.headers.get(\"Retry-After\", current_gemini_config.get('period_seconds', 60)))\n",
        "                        delay_val = retry_after_seconds + 5\n",
        "                    else:\n",
        "                         delay_val = current_gemini_config.get('period_seconds', 60) + (base_delay * (2 ** attempt))\n",
        "                except:\n",
        "                     delay_val = current_gemini_config.get('period_seconds', 60) + (base_delay * (2 ** attempt))\n",
        "\n",
        "            current_logger.error(f\"Gemini API error on attempt {attempt + 1}: {e} for prompt: {prompt[:200]}\", exc_info=True)\n",
        "            if attempt < max_retries:\n",
        "                current_logger.info(f\"Retrying Gemini API call (exception) in {delay_val} seconds...\")\n",
        "                time.sleep(delay_val)\n",
        "            else:\n",
        "                current_logger.error(f\"Gemini API call failed after {max_retries + 1} attempts for prompt: {prompt[:200]}\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def clean_json_response(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```json\"): text = text[7:]\n",
        "    if text.endswith(\"```\"): text = text[:-3]\n",
        "    if text.lower().startswith(\"json\") and \\\n",
        "       (text[4:].strip().startswith('{') or text[4:].strip().startswith('[')):\n",
        "        text = text[4:].strip()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_html(current_logger: logging.Logger, html_content: str, pmid: str) -> str:\n",
        "    if not html_content or not isinstance(html_content, str) or len(html_content.strip()) < 50:\n",
        "        current_logger.warning(f\"Insufficient or invalid HTML content for PMID {pmid} for any text extraction.\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        # FIX: Removed output_format, relying on trafilatura's default.\n",
        "        trafilatura_text = trafilatura.extract(\n",
        "            html_content, include_comments=False, include_tables=False, favor_recall=True\n",
        "        )\n",
        "        if trafilatura_text and len(trafilatura_text.strip()) > 200:\n",
        "            current_logger.info(f\"Trafilatura successfully extracted text for PMID {pmid}. Length: {len(trafilatura_text.strip())}\")\n",
        "            return trafilatura_text.strip()\n",
        "        else:\n",
        "            current_logger.info(f\"Trafilatura extracted insufficient text for PMID {pmid} (Length: {len(trafilatura_text.strip()) if trafilatura_text else 0}). Falling back to BeautifulSoup.\")\n",
        "    except Exception as e_traf:\n",
        "        current_logger.warning(f\"Trafilatura extraction failed for PMID {pmid}: {e_traf}. Falling back to BeautifulSoup.\", exc_info=False)\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        for tag_name in ['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'button', 'input', 'figure', 'figcaption', 'meta', 'link']:\n",
        "            for tag in soup.find_all(tag_name):\n",
        "                tag.decompose()\n",
        "        noisy_selectors = ['.sidebar', '#sidebar', '.related-posts', '.comments-area', '.site-sidebar',\n",
        "                           '.post-navigation', '.post-meta', '.breadcrumbs', '.site-header', '.site-footer',\n",
        "                           '.widget', 'div[class*=\"share\"]', 'div[class*=\"popup\"]', 'div[class*=\"cookie\"]',\n",
        "                           'div[id*=\"comment\"]', 'div[class*=\"banner\"]', 'div[class*=\"ad\"]']\n",
        "        for selector in noisy_selectors:\n",
        "            for el in soup.select(selector):\n",
        "                el.decompose()\n",
        "\n",
        "        main_content_selectors = ['article', 'main', '[role=\"main\"]', '.main-content', '.article-body',\n",
        "                                  '.entry-content', '#content', '.td-post-content', '.post-content', '.content',\n",
        "                                  'div[class*=\"article__body\"]', 'div[class*=\"story-content\"]', 'div.abstract', 'section.abstract']\n",
        "        for selector in main_content_selectors:\n",
        "            content_area = soup.select_one(selector)\n",
        "            if content_area:\n",
        "                text = content_area.get_text(separator=' ', strip=True)\n",
        "                if len(text) > 200:\n",
        "                    current_logger.info(f\"BeautifulSoup extracted text for PMID {pmid} using selector '{selector}'. Length: {len(text)}\")\n",
        "                    return text\n",
        "\n",
        "        current_logger.info(f\"No specific main content selector yielded substantial text (BeautifulSoup) for HTML of PMID {pmid}. Using stripped_strings from body or root.\")\n",
        "        body = soup.find('body')\n",
        "        target_element = body if body else soup\n",
        "\n",
        "        text_from_stripped = ' '.join(target_element.stripped_strings) if target_element else \"\"\n",
        "\n",
        "        if len(text_from_stripped) > 100:\n",
        "            current_logger.info(f\"BeautifulSoup stripped_strings (HTML) succeeded for PMID {pmid}. Length: {len(text_from_stripped)}\")\n",
        "            return text_from_stripped\n",
        "        else:\n",
        "            current_logger.warning(f\"BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID {pmid}. Length: {len(text_from_stripped)}\")\n",
        "            return \"\"\n",
        "    except Exception as e_bs:\n",
        "        current_logger.error(f\"Error during BeautifulSoup HTML parsing for PMID {pmid} (after trafilatura): {e_bs}\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_xml(current_logger: logging.Logger, xml_content: str, pmid: str) -> str:\n",
        "    if not xml_content or not isinstance(xml_content, str) or len(xml_content.strip()) < 50:\n",
        "        current_logger.warning(f\"Insufficient or invalid XML content for PMID {pmid}.\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(xml_content, 'xml')\n",
        "        tags_to_remove = ['table-wrap', 'fig', 'graphic', 'media', 'xref', 'disp-formula', 'inline-formula',\n",
        "                          'contrib-group', 'aff', 'author-notes', 'pub-history', 'permissions', 'license',\n",
        "                          'funding-group', 'related-article', 'front-stub', 'article-categories',\n",
        "                          'article-meta', 'journal-meta', 'ref-list', 'back', 'notes', 'app', 'app-group', 'ack']\n",
        "        for tag_name in tags_to_remove:\n",
        "            for tag in soup.find_all(tag_name):\n",
        "                tag.decompose()\n",
        "\n",
        "        body = soup.find('body')\n",
        "        if body:\n",
        "            text_content = body.get_text(separator=' ', strip=True)\n",
        "            if len(text_content) > 200:\n",
        "                current_logger.info(f\"Extracted text from XML 'body' for PMID {pmid}. Length: {len(text_content)}\")\n",
        "                return text_content\n",
        "\n",
        "        abstracts = soup.find_all('abstract')\n",
        "        abstract_text = \"\"\n",
        "        for abstract_tag in abstracts:\n",
        "            abstract_text += abstract_tag.get_text(separator=' ', strip=True) + \" \"\n",
        "        abstract_text = abstract_text.strip()\n",
        "\n",
        "        article_title_tag = soup.find('article-title')\n",
        "        title_text = article_title_tag.get_text(separator=' ', strip=True) if article_title_tag else \"\"\n",
        "\n",
        "        combined_text = (title_text + \" \" + abstract_text).strip()\n",
        "\n",
        "        if len(combined_text) > 100:\n",
        "             current_logger.info(f\"Extracted text from XML 'title' and/or 'abstract' for PMID {pmid}. Length: {len(combined_text)}\")\n",
        "             return combined_text\n",
        "\n",
        "        all_text = ' '.join(soup.stripped_strings)\n",
        "        if len(all_text) > 100:\n",
        "            current_logger.info(f\"Extracted text from XML (all stripped strings) for PMID {pmid}. Length: {len(all_text)}\")\n",
        "            return all_text\n",
        "\n",
        "        current_logger.warning(f\"Could not extract substantial text from XML for PMID {pmid} (final length: {len(all_text)}).\")\n",
        "        return \"\"\n",
        "    except Exception as e_xml:\n",
        "        current_logger.error(f\"Error during BeautifulSoup XML parsing for PMID {pmid}: {e_xml}\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "def get_text_from_content_item(current_logger: logging.Logger, pmid: str, content_item: Dict[str, Any]) -> str:\n",
        "    if not content_item or not isinstance(content_item, dict):\n",
        "        current_logger.warning(f\"Invalid content_item for PMID {pmid}.\")\n",
        "        return \"\"\n",
        "    content_type = content_item.get('type')\n",
        "    content_text = content_item.get('content')\n",
        "    final_url = content_item.get('final_url', 'N/A')\n",
        "\n",
        "    if not content_text or not isinstance(content_text, str):\n",
        "        current_logger.warning(f\"No content string or invalid content for PMID {pmid} from {final_url}. Type: {type(content_text)}\")\n",
        "        return \"\"\n",
        "    if content_type == 'html':\n",
        "        current_logger.info(f\"Attempting HTML text extraction for PMID {pmid} from {final_url}.\")\n",
        "        return extract_text_from_html(current_logger, content_text, pmid)\n",
        "    if content_type == 'xml':\n",
        "        current_logger.info(f\"Attempting XML text extraction for PMID {pmid} from {final_url}.\")\n",
        "        return extract_text_from_xml(current_logger, content_text, pmid)\n",
        "    if content_type == 'error' or content_text == \"Failed to retrieve\":\n",
        "        current_logger.warning(f\"Content for PMID {pmid} from {final_url} marked as error/failed.\")\n",
        "        return \"\"\n",
        "    current_logger.warning(f\"Unknown content type '{content_type}' for PMID {pmid} from {final_url}. Attempting HTML then XML extraction as fallback.\")\n",
        "    text = extract_text_from_html(current_logger, content_text, pmid)\n",
        "    if not text or len(text) < 100 :\n",
        "        text = extract_text_from_xml(current_logger, content_text, pmid)\n",
        "    return text\n",
        "\n",
        "# --- NEW: Process a single CSV row ---\n",
        "def process_single_csv_row_with_gemini(\n",
        "    current_logger: logging.Logger,\n",
        "    current_model_instance: genai.GenerativeModel,\n",
        "    pmid: str,\n",
        "    original_row_data: Dict[str, Any], # Single row from CSV as a dictionary\n",
        "    content_dict: Dict[str, Dict[str, Any]],\n",
        "    content_cache: Dict[str, str],\n",
        "    current_gemini_config: dict\n",
        ") -> Dict[str, Any]: # Returns a single augmented row dictionary\n",
        "\n",
        "    augmented_row = original_row_data.copy()\n",
        "    augmented_row.update({\"findings\": None, \"p_value\": None, \"population\": None,\n",
        "                          \"variant_details\": None, \"gene_association\": None})\n",
        "\n",
        "    paper_content_item = content_dict.get(pmid)\n",
        "    if not paper_content_item:\n",
        "        current_logger.warning(f\"No content item found in content_dict for PMID {pmid} (target row: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}). Skipping Gemini.\")\n",
        "        return augmented_row\n",
        "\n",
        "    if pmid not in content_cache:\n",
        "        text_for_llm = get_text_from_content_item(current_logger, pmid, paper_content_item)\n",
        "        content_cache[pmid] = text_for_llm\n",
        "    else:\n",
        "        text_for_llm = content_cache[pmid]\n",
        "\n",
        "    title_for_prompt = str(original_row_data.get('Title', 'No Title Provided')).strip()\n",
        "    current_text_length = len(text_for_llm) if isinstance(text_for_llm, str) else 0\n",
        "\n",
        "    if not text_for_llm or current_text_length < 100:\n",
        "        current_logger.warning(f\"Insufficient extracted text for PMID {pmid} (length: {current_text_length}). Using title for analysis of target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}.\")\n",
        "        text_for_llm = title_for_prompt\n",
        "        current_title_length = len(text_for_llm) if isinstance(text_for_llm, str) else 0\n",
        "        if not text_for_llm or current_title_length < 50:\n",
        "            current_logger.error(f\"Both extracted text and title are insufficient for PMID {pmid} (target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}). Skipping Gemini.\")\n",
        "            return augmented_row\n",
        "\n",
        "    if not isinstance(text_for_llm, str):\n",
        "        current_logger.error(f\"text_for_llm is not a string for PMID {pmid}. Type: {type(text_for_llm)}. Skipping Gemini.\")\n",
        "        return augmented_row\n",
        "\n",
        "    max_chars = current_gemini_config.get('max_content_chars', 150000)\n",
        "    if len(text_for_llm) > max_chars:\n",
        "        current_logger.info(f\"Truncating content for PMID {pmid} from {len(text_for_llm)} to {max_chars} chars for prompt.\")\n",
        "        text_for_llm = text_for_llm[:max_chars]\n",
        "\n",
        "    current_logger.info(f\"Content length for PMID {pmid} (target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}) to Gemini: {len(text_for_llm)}\")\n",
        "\n",
        "    # Construct prompt for a single target item\n",
        "    gene = str(original_row_data.get('Gene', 'N/A')).strip()\n",
        "    variant_val = original_row_data.get('Variant')\n",
        "    variant_str = str(variant_val).strip() if pd.notna(variant_val) and str(variant_val).strip() else \"not specified\"\n",
        "    alleles_val = original_row_data.get('Alleles')\n",
        "    alleles_str = str(alleles_val).strip() if pd.notna(alleles_val) and str(alleles_val).strip() else \"not specified\"\n",
        "\n",
        "    item_to_analyze_prompt = f\"\"\"The specific item to analyze from this paper is:\n",
        "- Gene: {gene}\n",
        "- Variant: {variant_str}\n",
        "- Alleles: {alleles_str}\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Analyze the scientific paper (PMID: {pmid}, Title: \"{title_for_prompt}\") provided below, focusing ONLY on the specific item listed.\n",
        "\n",
        "{item_to_analyze_prompt}\n",
        "\n",
        "Output a single JSON object. The JSON object MUST contain these exact keys: \"findings\" (main conclusion about the item), \"p_value\" (e.g., \"0.03\", \"<0.001\", \"NS\", or null if not found), \"population\" (study population description, or null if not found), \"variant_details\" (string summarizing variant info like type, location, predicted effect, if applicable; otherwise use the original variant string if provided and relevant, or null), \"gene_association\" (strength or type of association, e.g., \"strong association\", \"suggestive link\", \"no association\", \"functional impact noted\", or null if not found).\n",
        "If information for a key is not found for the specific item, use a JSON null value for that key.\n",
        "The \"variant_details\" field MUST be a string. Do NOT use markdown for the JSON output.\n",
        "\n",
        "Paper Content:\n",
        "{text_for_llm}\n",
        "\"\"\"\n",
        "    gemini_response = call_gemini_api_with_retries(current_logger, current_model_instance, prompt, current_gemini_config)\n",
        "\n",
        "    if not gemini_response or not hasattr(gemini_response, 'text') or not gemini_response.text:\n",
        "        current_logger.warning(f\"No valid response or text from Gemini for PMID {pmid}, target {gene}/{variant_str}.\")\n",
        "        return augmented_row\n",
        "\n",
        "    try:\n",
        "        cleaned_json_text = clean_json_response(gemini_response.text)\n",
        "        extracted_data_object = json.loads(cleaned_json_text) # Expect a single object now\n",
        "\n",
        "        if not isinstance(extracted_data_object, dict):\n",
        "            current_logger.error(f\"Gemini did not return a JSON object for PMID {pmid}, target {gene}/{variant_str}. Response: {cleaned_json_text[:500]}\")\n",
        "            return augmented_row\n",
        "\n",
        "        # Validate and update the single augmented_row\n",
        "        try:\n",
        "            if isinstance(extracted_data_object.get('variant_details'), dict):\n",
        "                current_logger.info(f\"Gemini returned dict for variant_details (PMID {pmid}, target {gene}/{variant_str}). Converting to string.\")\n",
        "                extracted_data_object['variant_details'] = json.dumps(extracted_data_object['variant_details'])\n",
        "\n",
        "            validated_item_data = ExtractedData(**extracted_data_object)\n",
        "\n",
        "            current_variant_details = validated_item_data.variant_details\n",
        "            if current_variant_details is None: # Fallback to original variant if Gemini provides null\n",
        "                original_variant_from_row = augmented_row.get('Variant')\n",
        "                if pd.notna(original_variant_from_row) and str(original_variant_from_row).strip() and str(original_variant_from_row).strip().lower() != \"not specified\":\n",
        "                    current_variant_details = str(original_variant_from_row).strip()\n",
        "\n",
        "            augmented_row.update({\n",
        "                \"findings\": validated_item_data.findings,\n",
        "                \"p_value\": validated_item_data.p_value,\n",
        "                \"population\": validated_item_data.population,\n",
        "                \"variant_details\": current_variant_details,\n",
        "                \"gene_association\": validated_item_data.gene_association\n",
        "            })\n",
        "            current_logger.info(f\"Successfully processed PMID {pmid}, target {gene}/{variant_str} with Gemini.\")\n",
        "        except ValidationError as e_val:\n",
        "            current_logger.error(f\"Pydantic validation error for PMID {pmid}, target {gene}/{variant_str}: {e_val}. JSON object: {extracted_data_object}\", exc_info=True)\n",
        "        except Exception as e_other:\n",
        "            current_logger.error(f\"Error updating row for PMID {pmid}, target {gene}/{variant_str}: {e_other}. JSON object: {extracted_data_object}\", exc_info=True)\n",
        "\n",
        "    except json.JSONDecodeError as e_json:\n",
        "        raw_response_text = gemini_response.text if gemini_response and hasattr(gemini_response, 'text') else 'Raw response text not available'\n",
        "        cleaned_text_for_error = locals().get('cleaned_json_text', raw_response_text)\n",
        "        current_logger.error(f\"JSON parsing error for PMID {pmid}, target {gene}/{variant_str}: {e_json}. Snippet: {cleaned_text_for_error[:500]}\", exc_info=True)\n",
        "    except Exception as e_gen:\n",
        "        current_logger.error(f\"General error processing Gemini response for PMID {pmid}, target {gene}/{variant_str}: {e_gen}\", exc_info=True)\n",
        "\n",
        "    return augmented_row\n",
        "\n",
        "\n",
        "def main(input_csv_path: Optional[str] = None):\n",
        "    global model\n",
        "    global gemini_config_settings\n",
        "    global logger # Use the globally configured logger\n",
        "\n",
        "    if not model:\n",
        "        logger.critical(\"Gemini model was not initialized globally. Terminating main function.\")\n",
        "        print(\"🚨 CRITICAL ERROR: Gemini model not initialized. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"--- Starting Step 3: Gemini Insight Extraction (Row-by-Row) ---\")\n",
        "\n",
        "    if not input_csv_path:\n",
        "        script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
        "        potential_dirs = [os.getcwd(), os.path.join(script_dir, \"data/output\"), os.path.join(script_dir, \"..\", \"data/output\")]\n",
        "        csv_files = []\n",
        "        for d_path in potential_dirs: # Renamed loop variable\n",
        "            if os.path.isdir(d_path):\n",
        "                csv_files.extend(glob.glob(os.path.join(d_path, 'pubmed_genetic_results_*.csv')))\n",
        "\n",
        "        if not csv_files:\n",
        "            logger.error(\"No pubmed_genetic_results_*.csv files found in standard locations.\")\n",
        "            print(\"Error: No input CSV (pubmed_genetic_results_*.csv) found.\")\n",
        "            return\n",
        "        input_csv_path = max(csv_files, key=os.path.getctime)\n",
        "\n",
        "    logger.info(f\"Using input CSV file: {input_csv_path}\")\n",
        "    print(f\"Using input CSV file: {input_csv_path}\")\n",
        "\n",
        "    content_dict_filename = \"content_dict.pkl.gz\"\n",
        "    content_dict_full_path = content_dict_filename\n",
        "    if not os.path.exists(content_dict_full_path):\n",
        "         script_dir_cd = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd() # Renamed variable\n",
        "         potential_paths_cd = [ # Renamed variable\n",
        "             os.path.join(script_dir_cd, \"data/output\", content_dict_filename),\n",
        "             os.path.join(script_dir_cd, \"..\", \"data/output\", content_dict_filename)\n",
        "         ]\n",
        "         for p_path_cd in potential_paths_cd: # Renamed loop variable\n",
        "             if os.path.exists(p_path_cd):\n",
        "                 content_dict_full_path = p_path_cd\n",
        "                 break\n",
        "\n",
        "    if not os.path.exists(content_dict_full_path):\n",
        "        logger.error(f\"Content dictionary '{content_dict_filename}' not found in standard locations.\")\n",
        "        print(f\"Error: Content dictionary '{content_dict_filename}' not found.\")\n",
        "        return\n",
        "    logger.info(f\"Using content dictionary: {content_dict_full_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "        if 'pmid' in df.columns and 'PMID' not in df.columns:\n",
        "            df.rename(columns={'pmid': 'PMID'}, inplace=True)\n",
        "        if 'PMID' not in df.columns:\n",
        "            logger.error(f\"CSV {input_csv_path} must contain 'PMID' or 'pmid' column.\")\n",
        "            print(f\"Error: CSV {input_csv_path} needs 'PMID' or 'pmid' column.\")\n",
        "            return\n",
        "        df['PMID'] = df['PMID'].astype(str)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading CSV {input_csv_path}: {e}\", exc_info=True)\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with gzip.open(content_dict_full_path, 'rb') as f_gz:\n",
        "            content_dict = {str(k): v for k, v in pickle.load(f_gz).items()}\n",
        "        logger.info(f\"Loaded {len(content_dict)} entries from {content_dict_full_path}\")\n",
        "        print(f\"Loaded {len(content_dict)} from {content_dict_full_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading {content_dict_full_path}: {e}\", exc_info=True)\n",
        "        print(f\"Error loading {content_dict_full_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    valid_pmids_with_content = set()\n",
        "    for pmid_key, item_data in content_dict.items():\n",
        "        if isinstance(item_data, dict) and \\\n",
        "           item_data.get('type') != 'error' and \\\n",
        "           isinstance(item_data.get('content'), str) and \\\n",
        "           item_data.get('content').strip():\n",
        "            valid_pmids_with_content.add(str(pmid_key))\n",
        "\n",
        "    original_row_count_df = len(df)\n",
        "    df_filtered = df[df['PMID'].isin(valid_pmids_with_content)].copy()\n",
        "    filtered_row_count_df = len(df_filtered) # Renamed variable\n",
        "\n",
        "    if filtered_row_count_df < original_row_count_df:\n",
        "        msg = (f\"Filtered CSV from {original_row_count_df} to {filtered_row_count_df} rows. \"\n",
        "               f\"{original_row_count_df - filtered_row_count_df} rows were removed because their PMIDs \"\n",
        "               f\"lacked valid, non-empty content in '{content_dict_full_path}'.\")\n",
        "        print(f\"INFO: {msg}\")\n",
        "        logger.info(msg)\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        msg = \"No rows remain to process after filtering for PMIDs with valid content. Exiting.\"\n",
        "        print(msg)\n",
        "        logging.info(msg)\n",
        "        return\n",
        "\n",
        "    # --- Testing Slice: Process only the first 120 rows ---\n",
        "    num_rows_to_test = 120\n",
        "    if len(df_filtered) > num_rows_to_test:\n",
        "        logger.info(f\"TESTING MODE: Processing only the first {num_rows_to_test} rows of the {len(df_filtered)} filtered rows.\")\n",
        "        print(f\"INFO: TESTING MODE - Will process only the first {num_rows_to_test} rows.\")\n",
        "        df_to_process = df_filtered.head(num_rows_to_test).copy()\n",
        "    else:\n",
        "        df_to_process = df_filtered.copy()\n",
        "    logger.info(f\"Number of CSV rows to process: {len(df_to_process)}\")\n",
        "    print(f\"Number of CSV rows to process: {len(df_to_process)}\")\n",
        "\n",
        "\n",
        "    max_gemini_workers = gemini_config_settings.get('max_gemini_workers', 1)\n",
        "\n",
        "    logger.info(f\"Starting Gemini processing for {len(df_to_process)} CSV rows. Using {max_gemini_workers} worker(s).\")\n",
        "    print(f\"Starting Gemini processing for {len(df_to_process)} CSV rows. Using {max_gemini_workers} worker(s).\")\n",
        "\n",
        "    all_processed_results_final: List[Dict[str, Any]] = []\n",
        "    text_extraction_cache: Dict[str, str] = {}\n",
        "    run_timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    tasks = []\n",
        "    logger.info(\"Creating tasks for individual CSV row processing...\")\n",
        "    # Each task now represents a single original CSV row.\n",
        "    # If multiple CSV rows share the same PMID, the paper's text will be fetched once (and cached),\n",
        "    # but a separate Gemini query will be made for each CSV row's specific gene/variant target.\n",
        "    for index, row_data_series in df_to_process.iterrows():\n",
        "        row_data_dict = row_data_series.to_dict()\n",
        "        pmid_val = str(row_data_dict['PMID']) # Renamed to avoid conflict\n",
        "        tasks.append((logger, model, pmid_val, row_data_dict, content_dict, text_extraction_cache, gemini_config_settings))\n",
        "\n",
        "    if max_gemini_workers > 1 and len(tasks) > 0 :\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_gemini_workers) as executor:\n",
        "            # Each task is now for a single row, so future_to_original_row might be more fitting\n",
        "            future_to_original_row = {\n",
        "                executor.submit(process_single_csv_row_with_gemini, *task_args): task_args[3] # task_args[3] is original_row_data (a dict)\n",
        "                for task_args in tasks\n",
        "            }\n",
        "\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_original_row), total=len(tasks), desc=\"Processing CSV rows (Gemini Parallel)\", unit=\"row\"):\n",
        "                original_row_if_error = future_to_original_row[future]\n",
        "                pmid_processed = str(original_row_if_error.get('PMID', \"UnknownPMID\"))\n",
        "                gene_processed = str(original_row_if_error.get('Gene', \"UnknownGene\"))\n",
        "\n",
        "                try:\n",
        "                    augmented_row_dict = future.result() # Expecting a single dict\n",
        "                    all_processed_results_final.append(augmented_row_dict)\n",
        "                except Exception as exc:\n",
        "                    logger.error(f\"PMID {pmid_processed}, Gene {gene_processed} generated an exception in ThreadPool: {exc}\", exc_info=True)\n",
        "                    # Append the original row with None for Gemini fields on error\n",
        "                    error_row = original_row_if_error.copy()\n",
        "                    error_row.update({\"findings\": None, \"p_value\": None, \"population\": None, \"variant_details\": None, \"gene_association\": None})\n",
        "                    all_processed_results_final.append(error_row)\n",
        "    else:\n",
        "        for task_args in tqdm(tasks, total=len(tasks), desc=\"Processing CSV rows (Gemini Sequential)\", unit=\"row\"):\n",
        "            augmented_row_dict = process_single_csv_row_with_gemini(*task_args)\n",
        "            all_processed_results_final.append(augmented_row_dict)\n",
        "\n",
        "    script_dir_for_output_main = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd() # Renamed variable\n",
        "    base_output_dir = os.path.join(script_dir_for_output_main, \"data/output\") if os.path.isdir(os.path.join(script_dir_for_output_main, \"data\")) else script_dir_for_output_main\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "    final_json_output_path = os.path.join(base_output_dir, f'gemini_extracted_row_by_row_{run_timestamp}.json')\n",
        "    try:\n",
        "        with open(final_json_output_path, 'w', encoding='utf-8') as f_final_json:\n",
        "            json.dump(all_processed_results_final, f_final_json, indent=4, ensure_ascii=False)\n",
        "        logger.info(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_json_output_path}\")\n",
        "        print(f\"\\nSaved final {len(all_processed_results_final)} augmented rows to {final_json_output_path}\")\n",
        "    except Exception as e_save_final_json:\n",
        "        logger.error(f\"Error saving final JSON results to {final_json_output_path}: {e_save_final_json}\", exc_info=True)\n",
        "        print(f\"\\nError saving final JSON results: {e_save_final_json}\")\n",
        "\n",
        "\n",
        "    final_csv_output_path = os.path.join(base_output_dir, f'gemini_extracted_row_by_row_{run_timestamp}.csv')\n",
        "    if all_processed_results_final:\n",
        "        try:\n",
        "            final_df = pd.DataFrame(all_processed_results_final)\n",
        "            original_cols_from_input_df = list(df.columns)\n",
        "            gemini_added_cols = [\"findings\", \"p_value\", \"population\", \"variant_details\", \"gene_association\"]\n",
        "\n",
        "            ordered_cols = [col for col in original_cols_from_input_df if col in final_df.columns]\n",
        "            for col in gemini_added_cols:\n",
        "                if col in final_df.columns and col not in ordered_cols:\n",
        "                    ordered_cols.append(col)\n",
        "            for col in final_df.columns:\n",
        "                if col not in ordered_cols:\n",
        "                    ordered_cols.append(col)\n",
        "\n",
        "            if ordered_cols:\n",
        "                final_df = final_df[ordered_cols]\n",
        "            else:\n",
        "                logger.warning(\"Could not determine column order for final CSV. Using default DataFrame order.\")\n",
        "\n",
        "            final_df.to_csv(final_csv_output_path, index=False, encoding='utf-8')\n",
        "            logger.info(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_csv_output_path}\")\n",
        "            print(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_csv_output_path}\")\n",
        "        except Exception as e_csv:\n",
        "            logger.error(f\"Could not save final results to CSV {final_csv_output_path}: {e_csv}\", exc_info=True)\n",
        "            print(f\"Error saving final results to CSV: {e_csv}. JSON available at {final_json_output_path}\")\n",
        "    else:\n",
        "        logger.info(\"No results processed to save to CSV.\")\n",
        "        print(\"No results processed to save to CSV.\")\n",
        "\n",
        "    logger.info(\"--- Step 3: Gemini Insight Extraction (Row-by-Row) Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0u7rMS-qnNE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
