{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ratelimit\n",
        "!pip install trafilatura\n",
        "import trafilatura\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field\n",
        "import google.generativeai as genai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZObWRXrapR4-",
        "outputId": "93f07ae3-f314-4b2f-9ace-a48fd270c94e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.4.26)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.2)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (1.9.3)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.4.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any, List # Added List\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field\n",
        "import google.generativeai as genai\n",
        "import concurrent.futures # For parallelization"
      ],
      "metadata": {
        "id": "5JMDXLK30ABh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import gzip\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "import google.generativeai as genai\n",
        "import concurrent.futures\n",
        "import trafilatura\n",
        "\n",
        "# --- Logging and API Configuration ---\n",
        "# WARNING: Hardcoding API keys is a significant security risk.\n",
        "GEMINI_API_KEY_HARDCODED = \"AIzaSyADS35cYhRoTNH6OCE2TpH4CMCJkeyTAMc\"  # Replace with your actual key\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='gemini_processing_errors.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "model = None\n",
        "gemini_config_settings = {}\n",
        "\n",
        "try:\n",
        "    # More robust check for placeholder/example key\n",
        "    is_placeholder_key = (\n",
        "        not GEMINI_API_KEY_HARDCODED or\n",
        "        GEMINI_API_KEY_HARDCODED.startswith(\"YOUR_\") or\n",
        "        GEMINI_API_KEY_HARDCODED == \"AIzaSyADS35cYhRoTNH6OCE2TpH4CMCJkeyTAMc\" # Example placeholder\n",
        "    )\n",
        "    if is_placeholder_key:\n",
        "        logger.critical(\"Using a placeholder or example Gemini API key. Please update it in the script.\")\n",
        "        print(\"ðŸš¨ CRITICAL WARNING: Using a placeholder or example Gemini API key. Please update it in the script.\")\n",
        "\n",
        "    genai.configure(api_key=GEMINI_API_KEY_HARDCODED)\n",
        "    logger.info(\"Gemini API configured with hardcoded key.\")\n",
        "\n",
        "    gemini_config_settings = {\n",
        "        'model_name': 'gemini-1.5-flash-latest',\n",
        "        'temperature': 0.1,\n",
        "        'max_output_tokens': 8192,\n",
        "        'request_timeout': 400,\n",
        "        'calls_per_period': 15,\n",
        "        'period_seconds': 60,\n",
        "        'max_gemini_workers': 1, # Default, can be changed for row-level parallelism\n",
        "        'max_content_chars': 150000\n",
        "    }\n",
        "    # If loading from a config file, update gemini_config_settings here.\n",
        "\n",
        "    model = genai.GenerativeModel(gemini_config_settings['model_name'])\n",
        "    logger.info(f\"Gemini model '{model.model_name}' initialized.\")\n",
        "    print(f\"Gemini model '{model.model_name}' initialized.\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.critical(f\"Failed to configure Gemini API or initialize model: {e}\", exc_info=True)\n",
        "    print(f\"ðŸš¨ CRITICAL ERROR: Failed to configure Gemini API or initialize model. Error: {e}\")\n",
        "\n",
        "# --- Data Models ---\n",
        "class ExtractedData(BaseModel):\n",
        "    findings: Optional[str] = Field(default=None)\n",
        "    p_value: Optional[str] = Field(default=None)\n",
        "    population: Optional[str] = Field(default=None)\n",
        "    variant_details: Optional[str] = Field(default=None)\n",
        "    gene_association: Optional[str] = Field(default=None)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "@sleep_and_retry\n",
        "@limits(calls=gemini_config_settings.get('calls_per_period', 15),\n",
        "        period=gemini_config_settings.get('period_seconds', 60))\n",
        "def call_gemini_api_with_retries(\n",
        "    current_logger: logging.Logger,\n",
        "    current_model_instance: genai.GenerativeModel, # Renamed for clarity\n",
        "    prompt: str,\n",
        "    current_gemini_config: dict,\n",
        "    max_retries=2,\n",
        "    base_delay=10\n",
        "):\n",
        "    if not current_model_instance:\n",
        "        current_logger.error(\"Gemini model is not initialized. Cannot make API call.\")\n",
        "        return None\n",
        "\n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            current_logger.debug(f\"Gemini API call attempt {attempt + 1} for prompt (first 100 chars): {prompt[:100]}\")\n",
        "            response = current_model_instance.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=current_gemini_config.get('temperature', 0.1),\n",
        "                    max_output_tokens=current_gemini_config.get('max_output_tokens', 8192),\n",
        "                    response_mime_type='application/json'\n",
        "                ),\n",
        "                request_options={'timeout': current_gemini_config.get('request_timeout', 400)}\n",
        "            )\n",
        "            current_logger.debug(f\"Gemini API response received. Safety: {response.prompt_feedback if response.prompt_feedback else 'N/A'}, Finish Reason: {response.candidates[0].finish_reason if response.candidates else 'N/A'}\")\n",
        "\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                block_msg = response.prompt_feedback.block_reason_message or \"No specific block message\"\n",
        "                current_logger.error(f\"Prompt blocked by Gemini API. Reason: {response.prompt_feedback.block_reason} - {block_msg}. Prompt: {prompt[:200]}\")\n",
        "                return None\n",
        "\n",
        "            if not response.candidates:\n",
        "                current_logger.warning(f\"No candidates in Gemini response for prompt: {prompt[:200]}\")\n",
        "                if attempt < max_retries:\n",
        "                    delay_val = base_delay * (2 ** attempt)\n",
        "                    current_logger.info(f\"Retrying due to no candidates in {delay_val}s...\")\n",
        "                    time.sleep(delay_val)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            if response.candidates[0].finish_reason != 1: # 1 is \"STOP\"\n",
        "                 current_logger.warning(f\"Gemini generation finished with reason: {response.candidates[0].finish_reason}. Content may be incomplete. Safety: {response.candidates[0].safety_ratings}\")\n",
        "\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            delay_val = base_delay * (2 ** attempt)\n",
        "            if \"429\" in str(e) or \"rate limit\" in str(e).lower():\n",
        "                current_logger.warning(f\"Gemini API rate limit error (429) on attempt {attempt + 1}: {e}. Applying longer backoff.\")\n",
        "                try:\n",
        "                    error_response = getattr(e, 'response', None)\n",
        "                    if error_response and hasattr(error_response, 'headers'):\n",
        "                        retry_after_seconds = int(error_response.headers.get(\"Retry-After\", current_gemini_config.get('period_seconds', 60)))\n",
        "                        delay_val = retry_after_seconds + 5\n",
        "                    else:\n",
        "                         delay_val = current_gemini_config.get('period_seconds', 60) + (base_delay * (2 ** attempt))\n",
        "                except:\n",
        "                     delay_val = current_gemini_config.get('period_seconds', 60) + (base_delay * (2 ** attempt))\n",
        "\n",
        "            current_logger.error(f\"Gemini API error on attempt {attempt + 1}: {e} for prompt: {prompt[:200]}\", exc_info=True)\n",
        "            if attempt < max_retries:\n",
        "                current_logger.info(f\"Retrying Gemini API call (exception) in {delay_val} seconds...\")\n",
        "                time.sleep(delay_val)\n",
        "            else:\n",
        "                current_logger.error(f\"Gemini API call failed after {max_retries + 1} attempts for prompt: {prompt[:200]}\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def clean_json_response(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```json\"): text = text[7:]\n",
        "    if text.endswith(\"```\"): text = text[:-3]\n",
        "    if text.lower().startswith(\"json\") and \\\n",
        "       (text[4:].strip().startswith('{') or text[4:].strip().startswith('[')):\n",
        "        text = text[4:].strip()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_html(current_logger: logging.Logger, html_content: str, pmid: str) -> str:\n",
        "    if not html_content or not isinstance(html_content, str) or len(html_content.strip()) < 50:\n",
        "        current_logger.warning(f\"Insufficient or invalid HTML content for PMID {pmid} for any text extraction.\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        # FIX: Removed output_format, relying on trafilatura's default.\n",
        "        trafilatura_text = trafilatura.extract(\n",
        "            html_content, include_comments=False, include_tables=False, favor_recall=True\n",
        "        )\n",
        "        if trafilatura_text and len(trafilatura_text.strip()) > 200:\n",
        "            current_logger.info(f\"Trafilatura successfully extracted text for PMID {pmid}. Length: {len(trafilatura_text.strip())}\")\n",
        "            return trafilatura_text.strip()\n",
        "        else:\n",
        "            current_logger.info(f\"Trafilatura extracted insufficient text for PMID {pmid} (Length: {len(trafilatura_text.strip()) if trafilatura_text else 0}). Falling back to BeautifulSoup.\")\n",
        "    except Exception as e_traf:\n",
        "        current_logger.warning(f\"Trafilatura extraction failed for PMID {pmid}: {e_traf}. Falling back to BeautifulSoup.\", exc_info=False)\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        for tag_name in ['script', 'style', 'header', 'footer', 'nav', 'aside', 'form', 'button', 'input', 'figure', 'figcaption', 'meta', 'link']:\n",
        "            for tag in soup.find_all(tag_name):\n",
        "                tag.decompose()\n",
        "        noisy_selectors = ['.sidebar', '#sidebar', '.related-posts', '.comments-area', '.site-sidebar',\n",
        "                           '.post-navigation', '.post-meta', '.breadcrumbs', '.site-header', '.site-footer',\n",
        "                           '.widget', 'div[class*=\"share\"]', 'div[class*=\"popup\"]', 'div[class*=\"cookie\"]',\n",
        "                           'div[id*=\"comment\"]', 'div[class*=\"banner\"]', 'div[class*=\"ad\"]']\n",
        "        for selector in noisy_selectors:\n",
        "            for el in soup.select(selector):\n",
        "                el.decompose()\n",
        "\n",
        "        main_content_selectors = ['article', 'main', '[role=\"main\"]', '.main-content', '.article-body',\n",
        "                                  '.entry-content', '#content', '.td-post-content', '.post-content', '.content',\n",
        "                                  'div[class*=\"article__body\"]', 'div[class*=\"story-content\"]', 'div.abstract', 'section.abstract']\n",
        "        for selector in main_content_selectors:\n",
        "            content_area = soup.select_one(selector)\n",
        "            if content_area:\n",
        "                text = content_area.get_text(separator=' ', strip=True)\n",
        "                if len(text) > 200:\n",
        "                    current_logger.info(f\"BeautifulSoup extracted text for PMID {pmid} using selector '{selector}'. Length: {len(text)}\")\n",
        "                    return text\n",
        "\n",
        "        current_logger.info(f\"No specific main content selector yielded substantial text (BeautifulSoup) for HTML of PMID {pmid}. Using stripped_strings from body or root.\")\n",
        "        body = soup.find('body')\n",
        "        target_element = body if body else soup\n",
        "\n",
        "        text_from_stripped = ' '.join(target_element.stripped_strings) if target_element else \"\"\n",
        "\n",
        "        if len(text_from_stripped) > 100:\n",
        "            current_logger.info(f\"BeautifulSoup stripped_strings (HTML) succeeded for PMID {pmid}. Length: {len(text_from_stripped)}\")\n",
        "            return text_from_stripped\n",
        "        else:\n",
        "            current_logger.warning(f\"BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID {pmid}. Length: {len(text_from_stripped)}\")\n",
        "            return \"\"\n",
        "    except Exception as e_bs:\n",
        "        current_logger.error(f\"Error during BeautifulSoup HTML parsing for PMID {pmid} (after trafilatura): {e_bs}\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_xml(current_logger: logging.Logger, xml_content: str, pmid: str) -> str:\n",
        "    if not xml_content or not isinstance(xml_content, str) or len(xml_content.strip()) < 50:\n",
        "        current_logger.warning(f\"Insufficient or invalid XML content for PMID {pmid}.\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(xml_content, 'xml')\n",
        "        tags_to_remove = ['table-wrap', 'fig', 'graphic', 'media', 'xref', 'disp-formula', 'inline-formula',\n",
        "                          'contrib-group', 'aff', 'author-notes', 'pub-history', 'permissions', 'license',\n",
        "                          'funding-group', 'related-article', 'front-stub', 'article-categories',\n",
        "                          'article-meta', 'journal-meta', 'ref-list', 'back', 'notes', 'app', 'app-group', 'ack']\n",
        "        for tag_name in tags_to_remove:\n",
        "            for tag in soup.find_all(tag_name):\n",
        "                tag.decompose()\n",
        "\n",
        "        body = soup.find('body')\n",
        "        if body:\n",
        "            text_content = body.get_text(separator=' ', strip=True)\n",
        "            if len(text_content) > 200:\n",
        "                current_logger.info(f\"Extracted text from XML 'body' for PMID {pmid}. Length: {len(text_content)}\")\n",
        "                return text_content\n",
        "\n",
        "        abstracts = soup.find_all('abstract')\n",
        "        abstract_text = \"\"\n",
        "        for abstract_tag in abstracts:\n",
        "            abstract_text += abstract_tag.get_text(separator=' ', strip=True) + \" \"\n",
        "        abstract_text = abstract_text.strip()\n",
        "\n",
        "        article_title_tag = soup.find('article-title')\n",
        "        title_text = article_title_tag.get_text(separator=' ', strip=True) if article_title_tag else \"\"\n",
        "\n",
        "        combined_text = (title_text + \" \" + abstract_text).strip()\n",
        "\n",
        "        if len(combined_text) > 100:\n",
        "             current_logger.info(f\"Extracted text from XML 'title' and/or 'abstract' for PMID {pmid}. Length: {len(combined_text)}\")\n",
        "             return combined_text\n",
        "\n",
        "        all_text = ' '.join(soup.stripped_strings)\n",
        "        if len(all_text) > 100:\n",
        "            current_logger.info(f\"Extracted text from XML (all stripped strings) for PMID {pmid}. Length: {len(all_text)}\")\n",
        "            return all_text\n",
        "\n",
        "        current_logger.warning(f\"Could not extract substantial text from XML for PMID {pmid} (final length: {len(all_text)}).\")\n",
        "        return \"\"\n",
        "    except Exception as e_xml:\n",
        "        current_logger.error(f\"Error during BeautifulSoup XML parsing for PMID {pmid}: {e_xml}\", exc_info=True)\n",
        "        return \"\"\n",
        "\n",
        "def get_text_from_content_item(current_logger: logging.Logger, pmid: str, content_item: Dict[str, Any]) -> str:\n",
        "    if not content_item or not isinstance(content_item, dict):\n",
        "        current_logger.warning(f\"Invalid content_item for PMID {pmid}.\")\n",
        "        return \"\"\n",
        "    content_type = content_item.get('type')\n",
        "    content_text = content_item.get('content')\n",
        "    final_url = content_item.get('final_url', 'N/A')\n",
        "\n",
        "    if not content_text or not isinstance(content_text, str):\n",
        "        current_logger.warning(f\"No content string or invalid content for PMID {pmid} from {final_url}. Type: {type(content_text)}\")\n",
        "        return \"\"\n",
        "    if content_type == 'html':\n",
        "        current_logger.info(f\"Attempting HTML text extraction for PMID {pmid} from {final_url}.\")\n",
        "        return extract_text_from_html(current_logger, content_text, pmid)\n",
        "    if content_type == 'xml':\n",
        "        current_logger.info(f\"Attempting XML text extraction for PMID {pmid} from {final_url}.\")\n",
        "        return extract_text_from_xml(current_logger, content_text, pmid)\n",
        "    if content_type == 'error' or content_text == \"Failed to retrieve\":\n",
        "        current_logger.warning(f\"Content for PMID {pmid} from {final_url} marked as error/failed.\")\n",
        "        return \"\"\n",
        "    current_logger.warning(f\"Unknown content type '{content_type}' for PMID {pmid} from {final_url}. Attempting HTML then XML extraction as fallback.\")\n",
        "    text = extract_text_from_html(current_logger, content_text, pmid)\n",
        "    if not text or len(text) < 100 :\n",
        "        text = extract_text_from_xml(current_logger, content_text, pmid)\n",
        "    return text\n",
        "\n",
        "# --- NEW: Process a single CSV row ---\n",
        "def process_single_csv_row_with_gemini(\n",
        "    current_logger: logging.Logger,\n",
        "    current_model_instance: genai.GenerativeModel,\n",
        "    pmid: str,\n",
        "    original_row_data: Dict[str, Any], # Single row from CSV as a dictionary\n",
        "    content_dict: Dict[str, Dict[str, Any]],\n",
        "    content_cache: Dict[str, str],\n",
        "    current_gemini_config: dict\n",
        ") -> Dict[str, Any]: # Returns a single augmented row dictionary\n",
        "\n",
        "    augmented_row = original_row_data.copy()\n",
        "    augmented_row.update({\"findings\": None, \"p_value\": None, \"population\": None,\n",
        "                          \"variant_details\": None, \"gene_association\": None})\n",
        "\n",
        "    paper_content_item = content_dict.get(pmid)\n",
        "    if not paper_content_item:\n",
        "        current_logger.warning(f\"No content item found in content_dict for PMID {pmid} (target row: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}). Skipping Gemini.\")\n",
        "        return augmented_row\n",
        "\n",
        "    if pmid not in content_cache:\n",
        "        text_for_llm = get_text_from_content_item(current_logger, pmid, paper_content_item)\n",
        "        content_cache[pmid] = text_for_llm\n",
        "    else:\n",
        "        text_for_llm = content_cache[pmid]\n",
        "\n",
        "    title_for_prompt = str(original_row_data.get('Title', 'No Title Provided')).strip()\n",
        "    current_text_length = len(text_for_llm) if isinstance(text_for_llm, str) else 0\n",
        "\n",
        "    if not text_for_llm or current_text_length < 100:\n",
        "        current_logger.warning(f\"Insufficient extracted text for PMID {pmid} (length: {current_text_length}). Using title for analysis of target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}.\")\n",
        "        text_for_llm = title_for_prompt\n",
        "        current_title_length = len(text_for_llm) if isinstance(text_for_llm, str) else 0\n",
        "        if not text_for_llm or current_title_length < 50:\n",
        "            current_logger.error(f\"Both extracted text and title are insufficient for PMID {pmid} (target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}). Skipping Gemini.\")\n",
        "            return augmented_row\n",
        "\n",
        "    if not isinstance(text_for_llm, str):\n",
        "        current_logger.error(f\"text_for_llm is not a string for PMID {pmid}. Type: {type(text_for_llm)}. Skipping Gemini.\")\n",
        "        return augmented_row\n",
        "\n",
        "    max_chars = current_gemini_config.get('max_content_chars', 150000)\n",
        "    if len(text_for_llm) > max_chars:\n",
        "        current_logger.info(f\"Truncating content for PMID {pmid} from {len(text_for_llm)} to {max_chars} chars for prompt.\")\n",
        "        text_for_llm = text_for_llm[:max_chars]\n",
        "\n",
        "    current_logger.info(f\"Content length for PMID {pmid} (target: {original_row_data.get('Gene')}/{original_row_data.get('Variant')}) to Gemini: {len(text_for_llm)}\")\n",
        "\n",
        "    # Construct prompt for a single target item\n",
        "    gene = str(original_row_data.get('Gene', 'N/A')).strip()\n",
        "    variant_val = original_row_data.get('Variant')\n",
        "    variant_str = str(variant_val).strip() if pd.notna(variant_val) and str(variant_val).strip() else \"not specified\"\n",
        "    alleles_val = original_row_data.get('Alleles')\n",
        "    alleles_str = str(alleles_val).strip() if pd.notna(alleles_val) and str(alleles_val).strip() else \"not specified\"\n",
        "\n",
        "    item_to_analyze_prompt = f\"\"\"The specific item to analyze from this paper is:\n",
        "- Gene: {gene}\n",
        "- Variant: {variant_str}\n",
        "- Alleles: {alleles_str}\n",
        "\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Analyze the scientific paper (PMID: {pmid}, Title: \"{title_for_prompt}\") provided below, focusing ONLY on the specific item listed.\n",
        "\n",
        "{item_to_analyze_prompt}\n",
        "\n",
        "Output a single JSON object. The JSON object MUST contain these exact keys: \"findings\" (main conclusion about the item), \"p_value\" (e.g., \"0.03\", \"<0.001\", \"NS\", or null if not found), \"population\" (study population description, or null if not found), \"variant_details\" (string summarizing variant info like type, location, predicted effect, if applicable; otherwise use the original variant string if provided and relevant, or null), \"gene_association\" (strength or type of association, e.g., \"strong association\", \"suggestive link\", \"no association\", \"functional impact noted\", or null if not found).\n",
        "If information for a key is not found for the specific item, use a JSON null value for that key.\n",
        "The \"variant_details\" field MUST be a string. Do NOT use markdown for the JSON output.\n",
        "\n",
        "Paper Content:\n",
        "{text_for_llm}\n",
        "\"\"\"\n",
        "    gemini_response = call_gemini_api_with_retries(current_logger, current_model_instance, prompt, current_gemini_config)\n",
        "\n",
        "    if not gemini_response or not hasattr(gemini_response, 'text') or not gemini_response.text:\n",
        "        current_logger.warning(f\"No valid response or text from Gemini for PMID {pmid}, target {gene}/{variant_str}.\")\n",
        "        return augmented_row\n",
        "\n",
        "    try:\n",
        "        cleaned_json_text = clean_json_response(gemini_response.text)\n",
        "        extracted_data_object = json.loads(cleaned_json_text) # Expect a single object now\n",
        "\n",
        "        if not isinstance(extracted_data_object, dict):\n",
        "            current_logger.error(f\"Gemini did not return a JSON object for PMID {pmid}, target {gene}/{variant_str}. Response: {cleaned_json_text[:500]}\")\n",
        "            return augmented_row\n",
        "\n",
        "        # Validate and update the single augmented_row\n",
        "        try:\n",
        "            if isinstance(extracted_data_object.get('variant_details'), dict):\n",
        "                current_logger.info(f\"Gemini returned dict for variant_details (PMID {pmid}, target {gene}/{variant_str}). Converting to string.\")\n",
        "                extracted_data_object['variant_details'] = json.dumps(extracted_data_object['variant_details'])\n",
        "\n",
        "            validated_item_data = ExtractedData(**extracted_data_object)\n",
        "\n",
        "            current_variant_details = validated_item_data.variant_details\n",
        "            if current_variant_details is None: # Fallback to original variant if Gemini provides null\n",
        "                original_variant_from_row = augmented_row.get('Variant')\n",
        "                if pd.notna(original_variant_from_row) and str(original_variant_from_row).strip() and str(original_variant_from_row).strip().lower() != \"not specified\":\n",
        "                    current_variant_details = str(original_variant_from_row).strip()\n",
        "\n",
        "            augmented_row.update({\n",
        "                \"findings\": validated_item_data.findings,\n",
        "                \"p_value\": validated_item_data.p_value,\n",
        "                \"population\": validated_item_data.population,\n",
        "                \"variant_details\": current_variant_details,\n",
        "                \"gene_association\": validated_item_data.gene_association\n",
        "            })\n",
        "            current_logger.info(f\"Successfully processed PMID {pmid}, target {gene}/{variant_str} with Gemini.\")\n",
        "        except ValidationError as e_val:\n",
        "            current_logger.error(f\"Pydantic validation error for PMID {pmid}, target {gene}/{variant_str}: {e_val}. JSON object: {extracted_data_object}\", exc_info=True)\n",
        "        except Exception as e_other:\n",
        "            current_logger.error(f\"Error updating row for PMID {pmid}, target {gene}/{variant_str}: {e_other}. JSON object: {extracted_data_object}\", exc_info=True)\n",
        "\n",
        "    except json.JSONDecodeError as e_json:\n",
        "        raw_response_text = gemini_response.text if gemini_response and hasattr(gemini_response, 'text') else 'Raw response text not available'\n",
        "        cleaned_text_for_error = locals().get('cleaned_json_text', raw_response_text)\n",
        "        current_logger.error(f\"JSON parsing error for PMID {pmid}, target {gene}/{variant_str}: {e_json}. Snippet: {cleaned_text_for_error[:500]}\", exc_info=True)\n",
        "    except Exception as e_gen:\n",
        "        current_logger.error(f\"General error processing Gemini response for PMID {pmid}, target {gene}/{variant_str}: {e_gen}\", exc_info=True)\n",
        "\n",
        "    return augmented_row\n",
        "\n",
        "\n",
        "def main(input_csv_path: Optional[str] = None):\n",
        "    global model\n",
        "    global gemini_config_settings\n",
        "    global logger # Use the globally configured logger\n",
        "\n",
        "    if not model:\n",
        "        logger.critical(\"Gemini model was not initialized globally. Terminating main function.\")\n",
        "        print(\"ðŸš¨ CRITICAL ERROR: Gemini model not initialized. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    logger.info(\"--- Starting Step 3: Gemini Insight Extraction (Row-by-Row) ---\")\n",
        "\n",
        "    if not input_csv_path:\n",
        "        script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
        "        potential_dirs = [os.getcwd(), os.path.join(script_dir, \"data/output\"), os.path.join(script_dir, \"..\", \"data/output\")]\n",
        "        csv_files = []\n",
        "        for d_path in potential_dirs: # Renamed loop variable\n",
        "            if os.path.isdir(d_path):\n",
        "                csv_files.extend(glob.glob(os.path.join(d_path, 'pubmed_genetic_results_*.csv')))\n",
        "\n",
        "        if not csv_files:\n",
        "            logger.error(\"No pubmed_genetic_results_*.csv files found in standard locations.\")\n",
        "            print(\"Error: No input CSV (pubmed_genetic_results_*.csv) found.\")\n",
        "            return\n",
        "        input_csv_path = max(csv_files, key=os.path.getctime)\n",
        "\n",
        "    logger.info(f\"Using input CSV file: {input_csv_path}\")\n",
        "    print(f\"Using input CSV file: {input_csv_path}\")\n",
        "\n",
        "    content_dict_filename = \"content_dict.pkl.gz\"\n",
        "    content_dict_full_path = content_dict_filename\n",
        "    if not os.path.exists(content_dict_full_path):\n",
        "         script_dir_cd = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd() # Renamed variable\n",
        "         potential_paths_cd = [ # Renamed variable\n",
        "             os.path.join(script_dir_cd, \"data/output\", content_dict_filename),\n",
        "             os.path.join(script_dir_cd, \"..\", \"data/output\", content_dict_filename)\n",
        "         ]\n",
        "         for p_path_cd in potential_paths_cd: # Renamed loop variable\n",
        "             if os.path.exists(p_path_cd):\n",
        "                 content_dict_full_path = p_path_cd\n",
        "                 break\n",
        "\n",
        "    if not os.path.exists(content_dict_full_path):\n",
        "        logger.error(f\"Content dictionary '{content_dict_filename}' not found in standard locations.\")\n",
        "        print(f\"Error: Content dictionary '{content_dict_filename}' not found.\")\n",
        "        return\n",
        "    logger.info(f\"Using content dictionary: {content_dict_full_path}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "        if 'pmid' in df.columns and 'PMID' not in df.columns:\n",
        "            df.rename(columns={'pmid': 'PMID'}, inplace=True)\n",
        "        if 'PMID' not in df.columns:\n",
        "            logger.error(f\"CSV {input_csv_path} must contain 'PMID' or 'pmid' column.\")\n",
        "            print(f\"Error: CSV {input_csv_path} needs 'PMID' or 'pmid' column.\")\n",
        "            return\n",
        "        df['PMID'] = df['PMID'].astype(str)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading CSV {input_csv_path}: {e}\", exc_info=True)\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with gzip.open(content_dict_full_path, 'rb') as f_gz:\n",
        "            content_dict = {str(k): v for k, v in pickle.load(f_gz).items()}\n",
        "        logger.info(f\"Loaded {len(content_dict)} entries from {content_dict_full_path}\")\n",
        "        print(f\"Loaded {len(content_dict)} from {content_dict_full_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading {content_dict_full_path}: {e}\", exc_info=True)\n",
        "        print(f\"Error loading {content_dict_full_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    valid_pmids_with_content = set()\n",
        "    for pmid_key, item_data in content_dict.items():\n",
        "        if isinstance(item_data, dict) and \\\n",
        "           item_data.get('type') != 'error' and \\\n",
        "           isinstance(item_data.get('content'), str) and \\\n",
        "           item_data.get('content').strip():\n",
        "            valid_pmids_with_content.add(str(pmid_key))\n",
        "\n",
        "    original_row_count_df = len(df)\n",
        "    df_filtered = df[df['PMID'].isin(valid_pmids_with_content)].copy()\n",
        "    filtered_row_count_df = len(df_filtered) # Renamed variable\n",
        "\n",
        "    if filtered_row_count_df < original_row_count_df:\n",
        "        msg = (f\"Filtered CSV from {original_row_count_df} to {filtered_row_count_df} rows. \"\n",
        "               f\"{original_row_count_df - filtered_row_count_df} rows were removed because their PMIDs \"\n",
        "               f\"lacked valid, non-empty content in '{content_dict_full_path}'.\")\n",
        "        print(f\"INFO: {msg}\")\n",
        "        logger.info(msg)\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        msg = \"No rows remain to process after filtering for PMIDs with valid content. Exiting.\"\n",
        "        print(msg)\n",
        "        logging.info(msg)\n",
        "        return\n",
        "\n",
        "    # --- Testing Slice: Process only the first 120 rows ---\n",
        "    num_rows_to_test = 120\n",
        "    if len(df_filtered) > num_rows_to_test:\n",
        "        logger.info(f\"TESTING MODE: Processing only the first {num_rows_to_test} rows of the {len(df_filtered)} filtered rows.\")\n",
        "        print(f\"INFO: TESTING MODE - Will process only the first {num_rows_to_test} rows.\")\n",
        "        df_to_process = df_filtered.head(num_rows_to_test).copy()\n",
        "    else:\n",
        "        df_to_process = df_filtered.copy()\n",
        "    logger.info(f\"Number of CSV rows to process: {len(df_to_process)}\")\n",
        "    print(f\"Number of CSV rows to process: {len(df_to_process)}\")\n",
        "\n",
        "\n",
        "    max_gemini_workers = gemini_config_settings.get('max_gemini_workers', 1)\n",
        "\n",
        "    logger.info(f\"Starting Gemini processing for {len(df_to_process)} CSV rows. Using {max_gemini_workers} worker(s).\")\n",
        "    print(f\"Starting Gemini processing for {len(df_to_process)} CSV rows. Using {max_gemini_workers} worker(s).\")\n",
        "\n",
        "    all_processed_results_final: List[Dict[str, Any]] = []\n",
        "    text_extraction_cache: Dict[str, str] = {}\n",
        "    run_timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    tasks = []\n",
        "    logger.info(\"Creating tasks for individual CSV row processing...\")\n",
        "    # Each task now represents a single original CSV row.\n",
        "    # If multiple CSV rows share the same PMID, the paper's text will be fetched once (and cached),\n",
        "    # but a separate Gemini query will be made for each CSV row's specific gene/variant target.\n",
        "    for index, row_data_series in df_to_process.iterrows():\n",
        "        row_data_dict = row_data_series.to_dict()\n",
        "        pmid_val = str(row_data_dict['PMID']) # Renamed to avoid conflict\n",
        "        tasks.append((logger, model, pmid_val, row_data_dict, content_dict, text_extraction_cache, gemini_config_settings))\n",
        "\n",
        "    if max_gemini_workers > 1 and len(tasks) > 0 :\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_gemini_workers) as executor:\n",
        "            # Each task is now for a single row, so future_to_original_row might be more fitting\n",
        "            future_to_original_row = {\n",
        "                executor.submit(process_single_csv_row_with_gemini, *task_args): task_args[3] # task_args[3] is original_row_data (a dict)\n",
        "                for task_args in tasks\n",
        "            }\n",
        "\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_original_row), total=len(tasks), desc=\"Processing CSV rows (Gemini Parallel)\", unit=\"row\"):\n",
        "                original_row_if_error = future_to_original_row[future]\n",
        "                pmid_processed = str(original_row_if_error.get('PMID', \"UnknownPMID\"))\n",
        "                gene_processed = str(original_row_if_error.get('Gene', \"UnknownGene\"))\n",
        "\n",
        "                try:\n",
        "                    augmented_row_dict = future.result() # Expecting a single dict\n",
        "                    all_processed_results_final.append(augmented_row_dict)\n",
        "                except Exception as exc:\n",
        "                    logger.error(f\"PMID {pmid_processed}, Gene {gene_processed} generated an exception in ThreadPool: {exc}\", exc_info=True)\n",
        "                    # Append the original row with None for Gemini fields on error\n",
        "                    error_row = original_row_if_error.copy()\n",
        "                    error_row.update({\"findings\": None, \"p_value\": None, \"population\": None, \"variant_details\": None, \"gene_association\": None})\n",
        "                    all_processed_results_final.append(error_row)\n",
        "    else:\n",
        "        for task_args in tqdm(tasks, total=len(tasks), desc=\"Processing CSV rows (Gemini Sequential)\", unit=\"row\"):\n",
        "            augmented_row_dict = process_single_csv_row_with_gemini(*task_args)\n",
        "            all_processed_results_final.append(augmented_row_dict)\n",
        "\n",
        "    script_dir_for_output_main = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd() # Renamed variable\n",
        "    base_output_dir = os.path.join(script_dir_for_output_main, \"data/output\") if os.path.isdir(os.path.join(script_dir_for_output_main, \"data\")) else script_dir_for_output_main\n",
        "    os.makedirs(base_output_dir, exist_ok=True)\n",
        "\n",
        "    final_json_output_path = os.path.join(base_output_dir, f'gemini_extracted_row_by_row_{run_timestamp}.json')\n",
        "    try:\n",
        "        with open(final_json_output_path, 'w', encoding='utf-8') as f_final_json:\n",
        "            json.dump(all_processed_results_final, f_final_json, indent=4, ensure_ascii=False)\n",
        "        logger.info(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_json_output_path}\")\n",
        "        print(f\"\\nSaved final {len(all_processed_results_final)} augmented rows to {final_json_output_path}\")\n",
        "    except Exception as e_save_final_json:\n",
        "        logger.error(f\"Error saving final JSON results to {final_json_output_path}: {e_save_final_json}\", exc_info=True)\n",
        "        print(f\"\\nError saving final JSON results: {e_save_final_json}\")\n",
        "\n",
        "\n",
        "    final_csv_output_path = os.path.join(base_output_dir, f'gemini_extracted_row_by_row_{run_timestamp}.csv')\n",
        "    if all_processed_results_final:\n",
        "        try:\n",
        "            final_df = pd.DataFrame(all_processed_results_final)\n",
        "            original_cols_from_input_df = list(df.columns)\n",
        "            gemini_added_cols = [\"findings\", \"p_value\", \"population\", \"variant_details\", \"gene_association\"]\n",
        "\n",
        "            ordered_cols = [col for col in original_cols_from_input_df if col in final_df.columns]\n",
        "            for col in gemini_added_cols:\n",
        "                if col in final_df.columns and col not in ordered_cols:\n",
        "                    ordered_cols.append(col)\n",
        "            for col in final_df.columns:\n",
        "                if col not in ordered_cols:\n",
        "                    ordered_cols.append(col)\n",
        "\n",
        "            if ordered_cols:\n",
        "                final_df = final_df[ordered_cols]\n",
        "            else:\n",
        "                logger.warning(\"Could not determine column order for final CSV. Using default DataFrame order.\")\n",
        "\n",
        "            final_df.to_csv(final_csv_output_path, index=False, encoding='utf-8')\n",
        "            logger.info(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_csv_output_path}\")\n",
        "            print(f\"Saved final {len(all_processed_results_final)} augmented rows to {final_csv_output_path}\")\n",
        "        except Exception as e_csv:\n",
        "            logger.error(f\"Could not save final results to CSV {final_csv_output_path}: {e_csv}\", exc_info=True)\n",
        "            print(f\"Error saving final results to CSV: {e_csv}. JSON available at {final_json_output_path}\")\n",
        "    else:\n",
        "        logger.info(\"No results processed to save to CSV.\")\n",
        "        print(\"No results processed to save to CSV.\")\n",
        "\n",
        "    logger.info(\"--- Step 3: Gemini Insight Extraction (Row-by-Row) Finished ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "OxfRSuWy4B3D",
        "outputId": "2ca8c7cd-a7a6-449b-cd9f-e954bfd308fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CRITICAL:__main__:Using a placeholder or example Gemini API key. Please update it in the script.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš¨ CRITICAL WARNING: Using a placeholder or example Gemini API key. Please update it in the script.\n",
            "Gemini model 'models/gemini-1.5-flash-latest' initialized.\n",
            "Using input CSV file: /content/pubmed_genetic_results_68a3f3d2.csv\n",
            "Loaded 923 from content_dict.pkl.gz\n",
            "INFO: Filtered CSV from 4695 to 3499 rows. 1196 rows were removed because their PMIDs lacked valid, non-empty content in 'content_dict.pkl.gz'.\n",
            "INFO: TESTING MODE - Will process only the first 120 rows.\n",
            "Number of CSV rows to process: 120\n",
            "Starting Gemini processing for 120 CSV rows. Using 1 worker(s).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing CSV rows (Gemini Sequential):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 57/120 [03:24<02:08,  2.04s/row]ERROR:tornado.access:503 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 533.38ms\n",
            "Processing CSV rows (Gemini Sequential):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 60/120 [03:31<02:10,  2.17s/row]WARNING:trafilatura.core:discarding data: None\n",
            "WARNING:__main__:BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID 18692774. Length: 0\n",
            "WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: Imd/nan.\n",
            "Processing CSV rows (Gemini Sequential):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 61/120 [04:00<09:48,  9.98s/row]WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: PGRP-LC/nan.\n",
            "Processing CSV rows (Gemini Sequential):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 62/120 [04:01<07:04,  7.33s/row]WARNING:__main__:Insufficient extracted text for PMID 18692774 (length: 0). Using title for analysis of target: commensal-gut/nan.\n",
            "Processing CSV rows (Gemini Sequential):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 96/120 [06:11<01:23,  3.47s/row]WARNING:trafilatura.core:discarding data: None\n",
            "WARNING:__main__:BeautifulSoup stripped_strings (HTML) also yielded insufficient text for PMID 16803893. Length: 0\n",
            "WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: AcPIM2/nan.\n",
            "Processing CSV rows (Gemini Sequential):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 97/120 [06:12<01:04,  2.81s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: AcPIM4-6/nan.\n",
            "Processing CSV rows (Gemini Sequential):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/120 [06:13<00:49,  2.25s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: Asp/nan.\n",
            "Processing CSV rows (Gemini Sequential):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 99/120 [06:14<00:38,  1.84s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PIM/nan.\n",
            "Processing CSV rows (Gemini Sequential):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 100/120 [06:15<00:32,  1.62s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PPM-dependent mannosyl-transferase/nan.\n",
            "Processing CSV rows (Gemini Sequential):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 101/120 [06:16<00:28,  1.49s/row]WARNING:__main__:Insufficient extracted text for PMID 16803893 (length: 0). Using title for analysis of target: PimE/nan.\n",
            "Processing CSV rows (Gemini Sequential): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [07:34<00:00,  3.79s/row]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved final 120 augmented rows to /content/gemini_extracted_row_by_row_20250531_174830.json\n",
            "Saved final 120 augmented rows to /content/gemini_extracted_row_by_row_20250531_174830.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0u7rMS-qnNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}