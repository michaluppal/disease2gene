{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caMn6VcscvAr",
        "outputId": "2a99f85c-da3c-41d5-c796-b7feb2c03357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"scispacyinstall\" - maybe you meant \"install\"\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz (120.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pathy>=0.3.5 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.3.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2025.4.26)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.2.1)\n",
            "Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: en_ner_bionlp13cg_md\n",
            "  Building wheel for en_ner_bionlp13cg_md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_ner_bionlp13cg_md: filename=en_ner_bionlp13cg_md-0.5.1-py3-none-any.whl size=120241137 sha256=32526dbe4f2bdcf13c78ef50e70cc4982b5aa94e883c766805b65188d2ad21a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/48/06/cb86feaf8cf8bb0d00a9465e788e3f19cc81e931c6a69b9859\n",
            "Successfully built en_ner_bionlp13cg_md\n",
            "Installing collected packages: wasabi, typer, smart-open, pydantic, pathlib-abc, blis, pathy, thinc, spacy, en_ner_bionlp13cg_md\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.3\n",
            "    Uninstalling typer-0.15.3:\n",
            "      Successfully uninstalled typer-0.15.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.6\n",
            "    Uninstalling spacy-3.8.6:\n",
            "      Successfully uninstalled spacy-3.8.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.60 requires pydantic>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
            "google-genai 1.16.1 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n",
            "langchain 0.3.25 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
            "albumentations 2.0.7 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 en_ner_bionlp13cg_md-0.5.1 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.22 smart-open-6.4.0 spacy-3.4.4 thinc-8.1.12 typer-0.7.0 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip scispacyinstall  requests\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
        "!pip install scispacy\n",
        "!pip install biopython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49OD4udTdCd6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import uuid\n",
        "import sys\n",
        "from Bio import Entrez, Medline\n",
        "from tqdm import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "import requests\n",
        "import spacy\n",
        "from Bio import Entrez, Medline\n",
        "import time\n",
        "import logging\n",
        "import csv\n",
        "import uuid\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mfcXu8b0ffmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbf029d-366e-4570-9df4-f0966b056f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_ner_bionlp13cg_md' (0.5.1) was trained with spaCy v3.4.1 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'hgnc_approved_genes.txt' not found. Attempting to download and create it...\n",
            "Attempting to download HGNC gene data from: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error downloading HGNC data: 404 Client Error: Not Found for url: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n",
            "ERROR:root:Failed to create 'hgnc_approved_genes.txt'.\n",
            "WARNING:root:Approved gene symbols file ('hgnc_approved_genes.txt') not found. Gene validation will be limited.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error downloading HGNC data: 404 Client Error: Not Found for url: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n",
            "Failed to create 'hgnc_approved_genes.txt'. Gene validation against HGNC list will be skipped.\n",
            "Warning: 'hgnc_approved_genes.txt' not found. SciSpaCy gene validation against HGNC list will be skipped.\n",
            "Paper 33106546 found in query.\n",
            "Searching PubMed...\n",
            "Found 923 papers initially from PubMed search.\n",
            "Fetching paper titles, authors, abstracts, year, and journal...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching title batches:   0%|          | 0/93 [00:00<?, ?batch/s]WARNING:root:Approved gene list (APPROVED_GENE_SYMBOLS) is empty. Keeping SciSpaCy genes without HGNC validation.\n",
            "Fetching title batches: 100%|██████████| 93/93 [01:47<00:00,  1.15s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched details for 923 papers.\n",
            "Fetching citation counts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching citation batches: 100%|██████████| 93/93 [00:45<00:00,  2.03batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching citation counts from Semantic Scholar...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching S2 citation batches:  23%|██▎       | 42/185 [03:12<10:54,  4.58s/batch]ERROR:root:Semantic Scholar API request timed out for batch starting with PMID:35571208, attempt 1.\n",
            "Fetching S2 citation batches:  71%|███████▏  | 132/185 [10:15<04:14,  4.81s/batch]ERROR:root:Semantic Scholar API request failed for batch PMID:26472029, attempt 1: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/batch?fields=citationCount,externalIds\n",
            "Fetching S2 citation batches: 100%|██████████| 185/185 [14:21<00:00,  4.66s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving linked NCBI genes in batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching NCBI genes batches: 100%|██████████| 93/93 [00:36<00:00,  2.54batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving linked NCBI variants in batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching NCBI variants batches: 100%|██████████| 93/93 [00:35<00:00,  2.59batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique NCBI-linked gene IDs to fetch names for: 1560\n",
            "Fetching gene names...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing gene name batches: 100%|██████████| 156/156 [04:42<00:00,  1.81s/batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique NCBI-linked variant IDs to fetch details for: 582\n",
            "Fetching variant details...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing variant detail batches: 100%|██████████| 59/59 [00:36<00:00,  1.61batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing and structuring results...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Structuring results: 100%|██████████| 923/923 [00:00<00:00, 52871.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results saved to pubmed_genetic_results_68a3f3d2.csv\n",
            "Script finished.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "HGNC_TSV_URL = \"https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\"\n",
        "APPROVED_GENE_SYMBOLS_FILE = \"hgnc_approved_genes.txt\"\n",
        "APPROVED_SYMBOL_COLUMN_NAME = \"symbol\"\n",
        "STATUS_COLUMN_NAME = \"status\"\n",
        "REQUIRED_STATUS = \"Approved\"\n",
        "APPROVED_GENE_SYMBOLS = set()\n",
        "\n",
        "ENTREZ_EMAIL = \"michal.uppal@gmail.com\"\n",
        "ENTREZ_API_KEY = \"be8c41829a4406a6d8a0f2d8ec242d9db108\"\n",
        "MAX_RESULTS = 5000\n",
        "BATCH_SIZE = 10\n",
        "RETRY_BATCH_SIZE = 3\n",
        "RETRIES = 3\n",
        "BASE_SLEEP = 0.1\n",
        "TARGET_AUTHOR = ['Casanova', 'Jean-Laurent Casanova', 'Casanova JL', 'Casanova J-L']\n",
        "PMIDS_TO_CHECK = ['33106546']\n",
        "CRITICAL_PMIDS = ['33106546']\n",
        "\n",
        "_GENE_BLACKLIST_TERMS = {\n",
        "    'SARS', 'MIS-C', 'PIMS', 'PIMS-TS', 'COVID', 'HLA', 'HIV', 'EBV',\n",
        "    'CONCLUSIONS', 'BACKGROUND', 'PATIENTS', 'METHODS', 'RESULTS', 'AND',\n",
        "    'SD', 'INTRODUCTION', 'ABSTRACT', 'DISCUSSION', 'GENE', 'VARIANT'\n",
        "}\n",
        "GENE_BLACKLIST = {term.upper() for term in _GENE_BLACKLIST_TERMS}\n",
        "\n",
        "VARIANT_KEYWORDS = {'variant', 'variants', 'mutation', 'mutations', 'allele', 'alleles'}\n",
        "QUERY = (\n",
        "    \"(Multisystem Inflammatory Syndrome in Children[MeSH Terms] OR \"\n",
        "    \"MIS-C[Title/Abstract] OR PIMS[Title/Abstract] OR PIMS-TS[Title/Abstract] OR \"\n",
        "    \"Paediatric Inflammatory Multisystem Syndrome[Title/Abstract] OR \"\n",
        "    \"Kawasaki Disease[MeSH Terms] OR Kawasaki Disease[Title/Abstract] OR Kawasaki Syndrome[Title/Abstract]) AND \"\n",
        "    \"(Genetic Predisposition to Disease[MeSH Terms] OR Polymorphism, Genetic[MeSH Terms] OR \"\n",
        "    \"Mutation[MeSH Terms] OR Genome-Wide Association Study[MeSH Terms] OR \"\n",
        "    \"genes[Title/Abstract] OR variants[Title/Abstract] OR genetic association[Title/Abstract] OR \"\n",
        "    \"polymorphism[Title/Abstract] OR mutation[Title/Abstract] OR gwas[Title/Abstract] OR \"\n",
        "    \"inborn errors[Title/Abstract] OR genetic defects[Title/Abstract] OR \"\n",
        "    \"genomics[Title/Abstract] OR gene expression[Title/Abstract] OR \"\n",
        "    f\"({' OR '.join(f'{author}[Author]' for author in TARGET_AUTHOR)}))\"\n",
        ")\n",
        "\n",
        "logging.basicConfig(filename='pubmed_errors.log', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "Entrez.email = ENTREZ_EMAIL\n",
        "if ENTREZ_API_KEY:\n",
        "    Entrez.api_key = ENTREZ_API_KEY\n",
        "\n",
        "nlp = None\n",
        "try:\n",
        "    nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "    logging.info(\"SciSpaCy model 'en_ner_bionlp13cg_md' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load SciSpaCy model 'en_ner_bionlp13cg_md': {e}.\")\n",
        "    print(f\"CRITICAL: Failed to load SciSpaCy model. Text processing for genes/variants will be skipped. Error: {e}\")\n",
        "\n",
        "def create_approved_gene_list_from_hgnc(url, output_file, symbol_column_name, status_column_name, required_status):\n",
        "    print(f\"Attempting to download HGNC gene data from: {url}\")\n",
        "    logging.info(f\"Attempting to download HGNC gene data from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        print(\"HGNC download successful.\")\n",
        "        logging.info(\"HGNC download successful.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading HGNC data: {e}\")\n",
        "        logging.error(f\"Error downloading HGNC data: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Processing HGNC TSV data to extract '{symbol_column_name}' where '{status_column_name}' is '{required_status}'...\")\n",
        "    lines = response.text.splitlines()\n",
        "    reader = csv.reader(lines, delimiter='\\t')\n",
        "\n",
        "    extracted_symbols = set()\n",
        "    header = None\n",
        "    symbol_column_index = -1\n",
        "    status_column_index = -1\n",
        "\n",
        "    try:\n",
        "        header = next(reader)\n",
        "\n",
        "        if symbol_column_name in header:\n",
        "            symbol_column_index = header.index(symbol_column_name)\n",
        "            logging.info(f\"Found '{symbol_column_name}' column at index {symbol_column_index} in HGNC data.\")\n",
        "        else:\n",
        "            print(f\"Error: Column '{symbol_column_name}' not found in HGNC header: {header}\")\n",
        "            logging.error(f\"Error: Column '{symbol_column_name}' not found in HGNC header: {header}\")\n",
        "            return False\n",
        "\n",
        "        if status_column_name in header:\n",
        "            status_column_index = header.index(status_column_name)\n",
        "            logging.info(f\"Found '{status_column_name}' column at index {status_column_index} in HGNC data.\")\n",
        "        else:\n",
        "            print(f\"Error: Column '{status_column_name}' not found in HGNC header: {header}\")\n",
        "            logging.error(f\"Error: Column '{status_column_name}' not found in HGNC header: {header}\")\n",
        "            return False\n",
        "\n",
        "        for row_number, row in enumerate(reader, 1):\n",
        "            if not row or all(not cell for cell in row):\n",
        "                logging.debug(f\"Skipping empty or blank row {row_number + 1} in HGNC data.\")\n",
        "                continue\n",
        "\n",
        "            if len(row) > symbol_column_index and len(row) > status_column_index:\n",
        "                current_status = row[status_column_index].strip()\n",
        "                if current_status == required_status:\n",
        "                    symbol = row[symbol_column_index].strip()\n",
        "                    if symbol:\n",
        "                        extracted_symbols.add(symbol.upper())\n",
        "                    else:\n",
        "                        logging.debug(f\"HGNC data: Row {row_number + 1} has status '{required_status}' but symbol is empty.\")\n",
        "            else:\n",
        "                logging.warning(f\"HGNC data: Row {row_number + 1} has insufficient columns. Length: {len(row)}, Row content (first 5 cells): {row[:5]}\")\n",
        "\n",
        "    except StopIteration:\n",
        "        print(\"Error: HGNC file appears to be empty after header or header is missing.\")\n",
        "        logging.error(\"Error: HGNC file appears to be empty after header or header is missing.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the HGNC TSV file: {e}\")\n",
        "        logging.error(f\"An error occurred while processing the HGNC TSV file: {e}\")\n",
        "        return False\n",
        "\n",
        "    if not extracted_symbols:\n",
        "        print(f\"No gene symbols with status '{required_status}' were extracted from HGNC data. Please check the file content and specified column names/status.\")\n",
        "        logging.warning(f\"No gene symbols with status '{required_status}' were extracted from HGNC data.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Extracted {len(extracted_symbols)} unique gene symbols with status '{required_status}'.\")\n",
        "    logging.info(f\"Extracted {len(extracted_symbols)} unique gene symbols with status '{required_status}'.\")\n",
        "\n",
        "    try:\n",
        "        with open(output_file, 'w') as f:\n",
        "            for symbol in sorted(list(extracted_symbols)):\n",
        "                f.write(symbol + \"\\n\")\n",
        "        print(f\"Successfully saved approved gene symbols to {output_file}\")\n",
        "        logging.info(f\"Successfully saved approved gene symbols to {output_file}\")\n",
        "        return True\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing HGNC gene symbols to output file {output_file}: {e}\")\n",
        "        logging.error(f\"Error writing HGNC gene symbols to output file {output_file}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_paper_in_query(pmid, query_to_check=QUERY):\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"pubmed\", term=query_to_check, retmax=10000)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "        is_found = pmid in record['IdList']\n",
        "        logging.info(f\"Paper {pmid} {'found' if is_found else 'not found'} in query.\")\n",
        "        print(f\"Paper {pmid} {'found' if is_found else 'not found'} in query.\")\n",
        "        return is_found\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking if paper {pmid} is in query: {e}\")\n",
        "        logging.error(f\"Error checking if paper {pmid} is in query: {e}\")\n",
        "        return False\n",
        "\n",
        "def search_pubmed(query_to_search=QUERY, max_r=MAX_RESULTS, num_retries=RETRIES):\n",
        "    print(\"Searching PubMed...\")\n",
        "    logging.info(f\"Searching PubMed with query (first 200 chars): {query_to_search[:200]}...\")\n",
        "    for attempt in range(num_retries):\n",
        "        try:\n",
        "            handle = Entrez.esearch(db=\"pubmed\", term=query_to_search, retmax=max_r)\n",
        "            record = Entrez.read(handle)\n",
        "            handle.close()\n",
        "            paper_ids = record['IdList']\n",
        "            logging.info(f\"Found {len(paper_ids)} papers in PubMed: {paper_ids[:10]}...\")\n",
        "            return paper_ids\n",
        "        except Exception as e:\n",
        "            logging.error(f\"PubMed search attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == num_retries - 1:\n",
        "                print(f\"Error searching PubMed after {num_retries} attempts: {e}\")\n",
        "            time.sleep(1 + attempt)\n",
        "    return []\n",
        "\n",
        "def fetch_paper_titles(paper_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    global APPROVED_GENE_SYMBOLS\n",
        "    paper_info = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching paper titles, authors, abstracts, year, and journal...\")\n",
        "    if not nlp:\n",
        "        logging.error(\"SciSpaCy model (nlp) not loaded. Gene/variant extraction from text will be skipped.\")\n",
        "\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=\"Fetching title batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch), rettype=\"medline\", retmode=\"text\")\n",
        "                records = list(Medline.parse(handle))\n",
        "                handle.close()\n",
        "                for record in records:\n",
        "                    if \"PMID\" in record and \"TI\" in record:\n",
        "                        authors = record.get(\"AU\", [])\n",
        "                        abstract = record.get(\"AB\", \"\")\n",
        "                        has_target_author = any(\n",
        "                            any(target.lower() in author.lower() for target in TARGET_AUTHOR)\n",
        "                            for author in authors\n",
        "                        ) if TARGET_AUTHOR else False\n",
        "\n",
        "                        dp_string = record.get(\"DP\", \"\")\n",
        "                        year = \"N/A\"\n",
        "                        if dp_string:\n",
        "                            match = re.search(r'\\b(\\d{4})\\b', dp_string)\n",
        "                            if match:\n",
        "                                year = match.group(1)\n",
        "\n",
        "                        journal = record.get(\"JT\", record.get(\"TA\", \"N/A\"))\n",
        "\n",
        "                        text_content = f\"{record.get('TI', '')} {abstract}\"\n",
        "\n",
        "                        validated_text_genes = []\n",
        "                        text_variants = []\n",
        "                        text_variant_context = []\n",
        "\n",
        "                        if nlp:\n",
        "                            doc = nlp(text_content)\n",
        "                            scispacy_genes = [ent.text for ent in doc.ents if ent.label_ == \"GENE_OR_GENE_PRODUCT\"]\n",
        "                            current_paper_validated_genes = set()\n",
        "\n",
        "                            for gene_candidate_original_case in scispacy_genes:\n",
        "                                gene_candidate_upper = gene_candidate_original_case.upper()\n",
        "                                if not (len(gene_candidate_original_case) > 2 and \\\n",
        "                                        not gene_candidate_original_case.isdigit() and \\\n",
        "                                        gene_candidate_upper not in GENE_BLACKLIST):\n",
        "                                    logging.debug(f\"Paper {record['PMID']}: Gene '{gene_candidate_original_case}' failed pre-filtering.\")\n",
        "                                    continue\n",
        "\n",
        "                                if APPROVED_GENE_SYMBOLS:\n",
        "                                    if gene_candidate_upper not in APPROVED_GENE_SYMBOLS:\n",
        "                                        logging.info(f\"Paper {record['PMID']}: SciSpaCy gene '{gene_candidate_original_case}' NOT in approved symbols. Discarding.\")\n",
        "                                        continue\n",
        "                                    current_paper_validated_genes.add(gene_candidate_original_case)\n",
        "                                else:\n",
        "                                    current_paper_validated_genes.add(gene_candidate_original_case)\n",
        "                                    if i == 0 and attempt == 0 and not getattr(fetch_paper_titles, 'warned_no_approved_list', False):\n",
        "                                        logging.warning(f\"Approved gene list (APPROVED_GENE_SYMBOLS) is empty. Keeping SciSpaCy genes without HGNC validation.\")\n",
        "                                        fetch_paper_titles.warned_no_approved_list = True\n",
        "\n",
        "                            validated_text_genes = sorted(list(current_paper_validated_genes))\n",
        "\n",
        "                            for token in doc:\n",
        "                                if token.text.lower() in VARIANT_KEYWORDS:\n",
        "                                    start = max(0, token.i - 5)\n",
        "                                    end = min(len(doc), token.i + 6)\n",
        "                                    window = doc[start:end]\n",
        "                                    variant_found = False\n",
        "                                    for t_variant in window:\n",
        "                                        if (t_variant.text.startswith('rs') and t_variant.text[2:].isdigit()) or \\\n",
        "                                           (t_variant.text.startswith('p.') and any(c.isdigit() for c in t_variant.text)):\n",
        "                                            text_variants.append(t_variant.text)\n",
        "                                            variant_found = True\n",
        "                                            break\n",
        "                                    if not variant_found:\n",
        "                                        text_variant_context.append(token.text)\n",
        "\n",
        "                            text_variants = sorted(list(set(v for v in text_variants if len(v) > 3 or v.startswith('rs'))))\n",
        "                            text_variant_context = sorted(list(set(c for c in text_variant_context if c in VARIANT_KEYWORDS)))\n",
        "\n",
        "                        paper_info[record[\"PMID\"]] = {\n",
        "                            \"title\": record[\"TI\"],\n",
        "                            \"authors\": authors,\n",
        "                            \"year\": year,\n",
        "                            \"journal\": journal,\n",
        "                            \"has_target_author\": has_target_author,\n",
        "                            \"abstract\": abstract,\n",
        "                            \"text_genes\": validated_text_genes,\n",
        "                            \"text_variants\": text_variants,\n",
        "                            \"text_variant_context\": text_variant_context\n",
        "                        }\n",
        "                        logging.debug(f\"Paper {record['PMID']} metadata: title='{record['TI']}', year='{year}', journal='{journal}', validated_text_genes={validated_text_genes}, text_variants={text_variants}\")\n",
        "                    else:\n",
        "                        logging.warning(f\"Skipping record in batch {i // b_size + 1}: Missing PMID or Title. Record PMID: {record.get('PMID', 'N/A')}\")\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch titles batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching title batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return paper_info, failed_ids\n",
        "\n",
        "def fetch_citation_counts(paper_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    citation_counts = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching citation counts...\")\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=\"Fetching citation batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.elink(dbfrom=\"pubmed\", db=\"pubmed\", linkname=\"pubmed_pubmed_citedin\", id=\",\".join(batch))\n",
        "                record = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for linkset in record:\n",
        "                    paper_id = linkset[\"IdList\"][0]\n",
        "                    try:\n",
        "                        if \"LinkSetDb\" in linkset and linkset[\"LinkSetDb\"]:\n",
        "                            citation_ids = [link[\"Id\"] for link in linkset[\"LinkSetDb\"][0][\"Link\"]]\n",
        "                            citation_counts[paper_id] = len(citation_ids)\n",
        "                        else:\n",
        "                            citation_counts[paper_id] = 0\n",
        "                    except Exception as e_inner:\n",
        "                        logging.error(f\"Error processing citation linkset for paper {paper_id}: {e_inner}\")\n",
        "                        citation_counts[paper_id] = 0\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch citations batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching citation batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return citation_counts, failed_ids\n",
        "\n",
        "def fetch_s2_citation_counts(pmids, s2_batch_size=5, num_retries=RETRIES):\n",
        "    s2_citation_counts = {}\n",
        "    print(\"Fetching citation counts from Semantic Scholar...\")\n",
        "    logging.info(f\"Starting Semantic Scholar citation fetch for {len(pmids)} PMIDs.\")\n",
        "\n",
        "    s2_api_url = \"https://api.semanticscholar.org/graph/v1/paper/batch?fields=citationCount,externalIds\"\n",
        "\n",
        "    pmids_with_prefix = [f\"PMID:{pmid}\" for pmid in pmids]\n",
        "\n",
        "    for i in tqdm(range(0, len(pmids_with_prefix), s2_batch_size), desc=\"Fetching S2 citation batches\", unit=\"batch\"):\n",
        "        batch_ids = pmids_with_prefix[i:i + s2_batch_size]\n",
        "        original_pmids_in_batch = pmids[i:i + s2_batch_size]\n",
        "\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                response = requests.post(s2_api_url, json={\"ids\": batch_ids}, timeout=20)\n",
        "\n",
        "                if response.status_code == 429:\n",
        "                    retry_after = response.headers.get(\"Retry-After\", \"60\")\n",
        "                    print(f\"Semantic Scholar rate limit hit. Retrying after {retry_after} seconds...\")\n",
        "                    logging.warning(f\"Semantic Scholar rate limit hit. Retrying after {retry_after} seconds for batch starting with {batch_ids[0]}\")\n",
        "                    time.sleep(int(retry_after) + 5)\n",
        "                    continue\n",
        "\n",
        "                response.raise_for_status()\n",
        "                data_batch = response.json()\n",
        "\n",
        "                for idx, paper_data in enumerate(data_batch):\n",
        "                    original_pmid = original_pmids_in_batch[idx]\n",
        "                    if paper_data:\n",
        "                        s2_citation_counts[original_pmid] = paper_data.get(\"citationCount\", 0)\n",
        "                    else:\n",
        "                        s2_citation_counts[original_pmid] = 0\n",
        "                        logging.info(f\"PMID {original_pmid} not found or no data in Semantic Scholar batch response.\")\n",
        "\n",
        "                time.sleep(3.5)\n",
        "                break\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                logging.error(f\"Semantic Scholar API request timed out for batch starting with {batch_ids[0]}, attempt {attempt + 1}.\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Semantic Scholar API request timed out after {num_retries} attempts for batch {batch_ids[0]}. Skipping this batch.\")\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                logging.error(f\"Semantic Scholar API request failed for batch {batch_ids[0]}, attempt {attempt + 1}: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Semantic Scholar API request failed after {num_retries} attempts for batch {batch_ids[0]}. Skipping batch.\")\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "            except Exception as e_general:\n",
        "                logging.error(f\"Unexpected error processing Semantic Scholar batch {batch_ids[0]}, attempt {attempt + 1}: {e_general}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "\n",
        "    logging.info(f\"Finished Semantic Scholar citation fetch. Found counts for {len(s2_citation_counts)} PMIDs.\")\n",
        "    return s2_citation_counts\n",
        "\n",
        "def fetch_linked_data(paper_ids, link_db, link_name, desc_name, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    paper_to_items = {}\n",
        "    failed_ids = []\n",
        "    print(f\"Retrieving linked {desc_name} in batches...\")\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=f\"Fetching {desc_name} batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.elink(dbfrom=\"pubmed\", db=link_db, linkname=link_name, id=\",\".join(batch))\n",
        "                record = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for linkset in record:\n",
        "                    paper_id = linkset[\"IdList\"][0]\n",
        "                    try:\n",
        "                        if \"LinkSetDb\" in linkset and linkset[\"LinkSetDb\"]:\n",
        "                            item_ids = [link[\"Id\"] for link in linkset[\"LinkSetDb\"][0][\"Link\"]]\n",
        "                            paper_to_items[paper_id] = item_ids\n",
        "                        else:\n",
        "                            paper_to_items[paper_id] = []\n",
        "                    except Exception as e_inner:\n",
        "                        logging.error(f\"Error processing {desc_name} linkset for paper {paper_id}: {e_inner}\")\n",
        "                        paper_to_items[paper_id] = []\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch linked {desc_name} batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching {desc_name} batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return paper_to_items, failed_ids\n",
        "\n",
        "\n",
        "def fetch_gene_names(gene_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    gene_names_map = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching gene names...\")\n",
        "    if not gene_ids: return {}, []\n",
        "    for i in tqdm(range(0, len(gene_ids), b_size), desc=\"Processing gene name batches\", unit=\"batch\"):\n",
        "        batch = gene_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"gene\", id=\",\".join(batch), retmode=\"xml\")\n",
        "                records = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for record_xml in records:\n",
        "                    gene_id = record_xml[\"Entrezgene_track-info\"][\"Gene-track\"][\"Gene-track_geneid\"]\n",
        "                    symbol = record_xml.get(\"Entrezgene_gene\", {}).get(\"Gene-ref\", {}).get(\"Gene-ref_locus\", \"Unknown Symbol\")\n",
        "                    gene_names_map[gene_id] = symbol\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch gene names batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching gene name batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return gene_names_map, failed_ids\n",
        "\n",
        "def fetch_variant_details(variant_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    variant_details_map = {}\n",
        "    failed_ids = []\n",
        "    ns = {'default': 'https://www.ncbi.nlm.nih.gov/SNP/docsum'}\n",
        "    print(\"Fetching variant details...\")\n",
        "    if not variant_ids: return {}, []\n",
        "    for i in tqdm(range(0, len(variant_ids), b_size), desc=\"Processing variant detail batches\", unit=\"batch\"):\n",
        "        batch = variant_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"snp\", id=\",\".join(batch), retmode=\"xml\")\n",
        "                xml_data = handle.read().decode('utf-8')\n",
        "                handle.close()\n",
        "                if not xml_data.strip():\n",
        "                    logging.warning(f\"Empty XML response for variant batch {i // b_size + 1}\")\n",
        "                    break\n",
        "                root = ET.fromstring(xml_data)\n",
        "                for doc in root.findall(\".//default:DocumentSummary\", namespaces=ns):\n",
        "                    snp_id_element = doc.find(\"default:SNP_ID\", namespaces=ns)\n",
        "                    variant_id = snp_id_element.text if snp_id_element is not None else \"UnknownID\"\n",
        "                    rsid = f\"rs{variant_id}\" if variant_id != \"UnknownID\" else \"UnknownRSID\"\n",
        "\n",
        "                    alleles = \"N/A\"\n",
        "                    global_mafs = doc.find(\"default:GLOBAL_MAFS\", namespaces=ns)\n",
        "                    if global_mafs is not None:\n",
        "                        alleles_set = set()\n",
        "                        maf_elements = global_mafs.findall(\"default:MAF\", namespaces=ns)\n",
        "                        for maf in maf_elements:\n",
        "                            freq = maf.find(\"default:FREQ\", namespaces=ns)\n",
        "                            if freq is not None and freq.text:\n",
        "                                try:\n",
        "                                    allele_part = freq.text.split('=')[0]\n",
        "                                    alleles_set.add(allele_part)\n",
        "                                except IndexError:\n",
        "                                    logging.warning(f\"Malformed FREQ text for variant {rsid}: {freq.text}\")\n",
        "                        if alleles_set:\n",
        "                             alleles = ','.join(sorted(alleles_set))\n",
        "\n",
        "                    gene_name_from_variant = \"\"\n",
        "                    genes_element = doc.find(\"default:GENES\", namespaces=ns)\n",
        "                    if genes_element is not None:\n",
        "                        gene_e = genes_element.find(\"default:GENE_E\", namespaces=ns)\n",
        "                        if gene_e is not None:\n",
        "                            name_element = gene_e.find(\"default:NAME\", namespaces=ns)\n",
        "                            if name_element is not None and name_element.text:\n",
        "                                gene_name_from_variant = name_element.text\n",
        "                    variant_details_map[variant_id] = {\"rsid\": rsid, \"alleles\": alleles, \"gene\": gene_name_from_variant}\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except ET.ParseError as e_parse:\n",
        "                logging.error(f\"XML ParseError for variant batch {i // b_size + 1}: {e_parse}. Data (first 500 chars): {xml_data[:500]}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch variant details batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching variant detail batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return variant_details_map, failed_ids\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not ENTREZ_EMAIL:\n",
        "        raise ValueError(\"ENTREZ_EMAIL must be set\")\n",
        "    if not QUERY:\n",
        "        raise ValueError(\"QUERY must be set\")\n",
        "    if not nlp:\n",
        "        print(\"WARNING: SciSpaCy model not loaded. Text-based gene/variant extraction will be limited.\")\n",
        "\n",
        "    if not os.path.exists(APPROVED_GENE_SYMBOLS_FILE):\n",
        "        print(f\"'{APPROVED_GENE_SYMBOLS_FILE}' not found. Attempting to download and create it...\")\n",
        "        if not create_approved_gene_list_from_hgnc(HGNC_TSV_URL, APPROVED_GENE_SYMBOLS_FILE,\n",
        "                                                    APPROVED_SYMBOL_COLUMN_NAME, STATUS_COLUMN_NAME, REQUIRED_STATUS):\n",
        "            print(f\"Failed to create '{APPROVED_GENE_SYMBOLS_FILE}'. Gene validation against HGNC list will be skipped.\")\n",
        "            logging.error(f\"Failed to create '{APPROVED_GENE_SYMBOLS_FILE}'.\")\n",
        "        else:\n",
        "             print(f\"Successfully created '{APPROVED_GENE_SYMBOLS_FILE}'.\")\n",
        "\n",
        "    if os.path.exists(APPROVED_GENE_SYMBOLS_FILE):\n",
        "        try:\n",
        "            with open(APPROVED_GENE_SYMBOLS_FILE, 'r') as f:\n",
        "                APPROVED_GENE_SYMBOLS = {line.strip().upper() for line in f if line.strip()}\n",
        "            if APPROVED_GENE_SYMBOLS:\n",
        "                logging.info(f\"Successfully loaded {len(APPROVED_GENE_SYMBOLS)} approved gene symbols from {APPROVED_GENE_SYMBOLS_FILE}.\")\n",
        "                print(f\"Loaded {len(APPROVED_GENE_SYMBOLS)} approved gene symbols for validation.\")\n",
        "            else:\n",
        "                logging.warning(f\"No gene symbols loaded from {APPROVED_GENE_SYMBOLS_FILE} (file might be empty).\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading gene symbols from {APPROVED_GENE_SYMBOLS_FILE}: {e}\")\n",
        "            print(f\"Error loading gene symbols from {APPROVED_GENE_SYMBOLS_FILE}. Validation may be affected.\")\n",
        "    else:\n",
        "        logging.warning(f\"Approved gene symbols file ('{APPROVED_GENE_SYMBOLS_FILE}') not found. Gene validation will be limited.\")\n",
        "        print(f\"Warning: '{APPROVED_GENE_SYMBOLS_FILE}' not found. SciSpaCy gene validation against HGNC list will be skipped.\")\n",
        "\n",
        "    for pmid_to_check in PMIDS_TO_CHECK:\n",
        "        check_paper_in_query(pmid_to_check)\n",
        "\n",
        "    paper_ids_list = search_pubmed()\n",
        "    print(f\"Found {len(paper_ids_list)} papers initially from PubMed search.\")\n",
        "\n",
        "    initial_pmid_count = len(paper_ids_list)\n",
        "    paper_ids_set = set(paper_ids_list)\n",
        "    for critical_pmid in CRITICAL_PMIDS:\n",
        "        if critical_pmid not in paper_ids_set:\n",
        "            paper_ids_set.add(critical_pmid)\n",
        "            logging.info(f\"Manually added critical PMID {critical_pmid} to paper_ids_set\")\n",
        "    paper_ids_list = list(paper_ids_set)\n",
        "    if len(paper_ids_list) > initial_pmid_count:\n",
        "        print(f\"Added {len(paper_ids_list) - initial_pmid_count} critical PMIDs. Total papers to process: {len(paper_ids_list)}\")\n",
        "\n",
        "    fetch_paper_titles.warned_no_approved_list = False\n",
        "    paper_info_dict, failed_title_ids = fetch_paper_titles(paper_ids_list)\n",
        "    if failed_title_ids:\n",
        "        print(f\"Retrying {len(failed_title_ids)} failed title IDs with batch size {RETRY_BATCH_SIZE}...\")\n",
        "        retry_info, _ = fetch_paper_titles(failed_title_ids, b_size=RETRY_BATCH_SIZE)\n",
        "        paper_info_dict.update(retry_info)\n",
        "    logging.info(f\"Retrieved titles, authors, and abstracts for {len(paper_info_dict)} papers.\")\n",
        "    print(f\"Fetched details for {len(paper_info_dict)} papers.\")\n",
        "\n",
        "    processed_paper_ids = list(paper_info_dict.keys())\n",
        "    citation_counts_dict, failed_citation_ids = fetch_citation_counts(processed_paper_ids)\n",
        "    if failed_citation_ids:\n",
        "        print(f\"Retrying {len(failed_citation_ids)} failed citation IDs...\")\n",
        "        retry_citations, _ = fetch_citation_counts(failed_citation_ids, b_size=RETRY_BATCH_SIZE)\n",
        "        citation_counts_dict.update(retry_citations)\n",
        "    logging.info(f\"Retrieved citation counts for {len(citation_counts_dict)} papers.\")\n",
        "\n",
        "    s2_citation_counts_dict = {}\n",
        "    if processed_paper_ids:\n",
        "        s2_citation_counts_dict = fetch_s2_citation_counts(processed_paper_ids)\n",
        "    logging.info(f\"Retrieved Semantic Scholar citation counts for {len(s2_citation_counts_dict)} papers.\")\n",
        "\n",
        "    paper_to_ncbi_genes, failed_gene_ids = fetch_linked_data(processed_paper_ids, \"gene\", \"pubmed_gene\", \"NCBI genes\")\n",
        "    paper_to_ncbi_variants, failed_variant_ids = fetch_linked_data(processed_paper_ids, \"snp\", \"pubmed_snp\", \"NCBI variants\")\n",
        "\n",
        "    all_ncbi_gene_ids = list(set(sum(paper_to_ncbi_genes.values(), [])))\n",
        "    print(f\"Total unique NCBI-linked gene IDs to fetch names for: {len(all_ncbi_gene_ids)}\")\n",
        "    gene_names_dict = {}\n",
        "    if all_ncbi_gene_ids:\n",
        "        gene_names_dict, _ = fetch_gene_names(all_ncbi_gene_ids)\n",
        "        logging.info(f\"Retrieved names for {len(gene_names_dict)} NCBI-linked genes.\")\n",
        "\n",
        "    all_ncbi_variant_ids = list(set(sum(paper_to_ncbi_variants.values(), [])))\n",
        "    print(f\"Total unique NCBI-linked variant IDs to fetch details for: {len(all_ncbi_variant_ids)}\")\n",
        "    variant_details_dict = {}\n",
        "    if all_ncbi_variant_ids:\n",
        "        variant_details_dict, _ = fetch_variant_details(all_ncbi_variant_ids)\n",
        "        logging.info(f\"Retrieved details for {len(variant_details_dict)} NCBI-linked variants.\")\n",
        "\n",
        "    results_list = []\n",
        "    print(\"\\nProcessing and structuring results...\")\n",
        "    for pmid, info_data in tqdm(paper_info_dict.items(), desc=\"Structuring results\"):\n",
        "        title = info_data.get(\"title\", \"N/A\")\n",
        "        year = info_data.get(\"year\", \"N/A\")\n",
        "        journal = info_data.get(\"journal\", \"N/A\")\n",
        "        has_target_author = info_data.get(\"has_target_author\", False)\n",
        "\n",
        "        text_genes = info_data.get(\"text_genes\", [])\n",
        "        text_variants = info_data.get(\"text_variants\", [])\n",
        "        text_variant_context = info_data.get(\"text_variant_context\", [])\n",
        "\n",
        "        linked_gene_ids_for_paper = paper_to_ncbi_genes.get(pmid, [])\n",
        "        ncbi_gene_symbols = [gene_names_dict.get(gid, f\"ID:{gid}\") for gid in linked_gene_ids_for_paper \\\n",
        "                             if gene_names_dict.get(gid) and gene_names_dict.get(gid) != \"Unknown Symbol\"]\n",
        "        ncbi_gene_symbols = sorted(list(set(s for s in ncbi_gene_symbols if s)))\n",
        "\n",
        "        linked_variant_ids_for_paper = paper_to_ncbi_variants.get(pmid, [])\n",
        "\n",
        "        pubmed_citation_count = citation_counts_dict.get(pmid, 0)\n",
        "        s2_citation_count = s2_citation_counts_dict.get(pmid, 0)\n",
        "\n",
        "\n",
        "        if (text_genes or text_variants or text_variant_context) and \\\n",
        "           not ncbi_gene_symbols and not linked_variant_ids_for_paper:\n",
        "            logging.info(f\"Paper {pmid} has text-derived entities but no NCBI-linked genes/variants.\")\n",
        "\n",
        "        processed_genes_for_this_paper = set()\n",
        "\n",
        "        for gene_symbol in ncbi_gene_symbols:\n",
        "            results_list.append({\n",
        "                'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': gene_symbol,\n",
        "                'Variant': '', 'VariantContext': 'NCBI_Linked_Gene', 'Alleles': '',\n",
        "                'Citations_PubMed': pubmed_citation_count,\n",
        "                'Citations_S2': s2_citation_count,\n",
        "                'AuthorFlag': has_target_author\n",
        "            })\n",
        "            processed_genes_for_this_paper.add(gene_symbol.upper())\n",
        "\n",
        "        for vid in linked_variant_ids_for_paper:\n",
        "            v_info = variant_details_dict.get(vid, {\"rsid\": f\"ID:{vid}\", \"alleles\": \"N/A\", \"gene\": \"\"})\n",
        "            variant_gene = v_info['gene']\n",
        "            if variant_gene and variant_gene != \"Unknown Symbol\":\n",
        "                results_list.append({\n",
        "                    'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': variant_gene,\n",
        "                    'Variant': v_info['rsid'], 'VariantContext': 'NCBI_Linked_Variant',\n",
        "                    'Alleles': v_info['alleles'],\n",
        "                    'Citations_PubMed': pubmed_citation_count,\n",
        "                    'Citations_S2': s2_citation_count,\n",
        "                    'AuthorFlag': has_target_author\n",
        "                })\n",
        "                processed_genes_for_this_paper.add(variant_gene.upper())\n",
        "            elif v_info['rsid'] != \"UnknownRSID\":\n",
        "                 results_list.append({\n",
        "                    'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': f\"FromVariant_{v_info['rsid']}\",\n",
        "                    'Variant': v_info['rsid'], 'VariantContext': 'NCBI_Linked_Variant_GeneUnknown',\n",
        "                    'Alleles': v_info['alleles'],\n",
        "                    'Citations_PubMed': pubmed_citation_count,\n",
        "                    'Citations_S2': s2_citation_count,\n",
        "                    'AuthorFlag': has_target_author\n",
        "                })\n",
        "\n",
        "        if text_genes:\n",
        "            for gene in text_genes:\n",
        "                if gene.upper() not in processed_genes_for_this_paper:\n",
        "                    best_variant_for_text_gene = text_variants[0] if text_variants else \"\"\n",
        "                    context_str = \"Text_Derived_Gene\"\n",
        "                    if best_variant_for_text_gene:\n",
        "                        context_str = \"Text_Gene_With_Text_Variant\"\n",
        "                    elif text_variant_context:\n",
        "                        context_str = f\"Text_Gene_Context:{text_variant_context[0]}\"\n",
        "\n",
        "                    results_list.append({\n",
        "                        'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': gene,\n",
        "                        'Variant': best_variant_for_text_gene,\n",
        "                        'VariantContext': context_str,\n",
        "                        'Alleles': '',\n",
        "                        'Citations_PubMed': pubmed_citation_count,\n",
        "                        'Citations_S2': s2_citation_count,\n",
        "                        'AuthorFlag': has_target_author\n",
        "                    })\n",
        "\n",
        "        paper_has_entry_in_this_iteration = False\n",
        "        if processed_genes_for_this_paper or linked_variant_ids_for_paper or (text_genes and any(g.upper() not in processed_genes_for_this_paper for g in text_genes)):\n",
        "             paper_has_entry_in_this_iteration = True\n",
        "\n",
        "\n",
        "        if not paper_has_entry_in_this_iteration:\n",
        "            placeholder_gene = \"NoGeneData\"\n",
        "            placeholder_context = \"NoGeneticInfoFound\"\n",
        "            if nlp and not text_genes and not text_variants and not text_variant_context:\n",
        "                placeholder_gene = \"SciSpaCyHit_Filtered\"\n",
        "                placeholder_context = \"SciSpaCyHit_NotValidatedOrLinked\"\n",
        "\n",
        "            results_list.append({\n",
        "                'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': placeholder_gene,\n",
        "                'Variant': '', 'VariantContext': placeholder_context, 'Alleles': '',\n",
        "                'Citations_PubMed': pubmed_citation_count,\n",
        "                'Citations_S2': s2_citation_count,\n",
        "                'AuthorFlag': has_target_author\n",
        "            })\n",
        "            logging.info(f\"Paper {pmid} ('{title[:50]}...') added with placeholder: Gene='{placeholder_gene}', Context='{placeholder_context}'.\")\n",
        "\n",
        "    results_list.sort(key=lambda x: (x.get('Citations_S2', 0), x.get('Citations_PubMed', 0), x.get('PMID', '')), reverse=True)\n",
        "\n",
        "    if results_list:\n",
        "        fieldnames = ['PMID', 'Title', 'Year', 'Journal', 'Gene', 'Variant', 'VariantContext',\n",
        "                      'Alleles', 'Citations_PubMed', 'Citations_S2', 'AuthorFlag']\n",
        "        csv_file = f\"pubmed_genetic_results_{uuid.uuid4().hex[:8]}.csv\"\n",
        "        with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile_obj:\n",
        "            writer = csv.DictWriter(csvfile_obj, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(results_list)\n",
        "        print(f\"\\nResults saved to {csv_file}\")\n",
        "        logging.info(f\"Saved {len(results_list)} rows to {csv_file}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to save.\")\n",
        "        logging.info(\"No results generated to save to CSV.\")\n",
        "\n",
        "    print(\"Script finished.\")\n",
        "    logging.info(\"Script finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3P-ege8MOOO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}