{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caMn6VcscvAr",
        "outputId": "2a99f85c-da3c-41d5-c796-b7feb2c03357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR: unknown command \"scispacyinstall\" - maybe you meant \"install\"\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz (120.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy<3.5.0,>=3.4.1 (from en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.9)\n",
            "Collecting thinc<8.2.0,>=8.1.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.10)\n",
            "Collecting typer<0.8.0,>=0.3.0 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading typer-0.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pathy>=0.3.5 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.32.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.3.0)\n",
            "Collecting pathlib-abc==0.1.1 (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (2025.4.26)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (8.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.5.0,>=3.4.1->en_ner_bionlp13cg_md==0.5.1) (1.2.1)\n",
            "Downloading spacy-3.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
            "Downloading pydantic-1.10.22-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.1.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (917 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.4/917.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: en_ner_bionlp13cg_md\n",
            "  Building wheel for en_ner_bionlp13cg_md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_ner_bionlp13cg_md: filename=en_ner_bionlp13cg_md-0.5.1-py3-none-any.whl size=120241137 sha256=32526dbe4f2bdcf13c78ef50e70cc4982b5aa94e883c766805b65188d2ad21a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/48/06/cb86feaf8cf8bb0d00a9465e788e3f19cc81e931c6a69b9859\n",
            "Successfully built en_ner_bionlp13cg_md\n",
            "Installing collected packages: wasabi, typer, smart-open, pydantic, pathlib-abc, blis, pathy, thinc, spacy, en_ner_bionlp13cg_md\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.3\n",
            "    Uninstalling typer-0.15.3:\n",
            "      Successfully uninstalled typer-0.15.3\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.6\n",
            "    Uninstalling spacy-3.8.6:\n",
            "      Successfully uninstalled spacy-3.8.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.60 requires pydantic>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
            "google-genai 1.16.1 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n",
            "langchain 0.3.25 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.22 which is incompatible.\n",
            "albumentations 2.0.7 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 en_ner_bionlp13cg_md-0.5.1 pathlib-abc-0.1.1 pathy-0.11.0 pydantic-1.10.22 smart-open-6.4.0 spacy-3.4.4 thinc-8.1.12 typer-0.7.0 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip scispacyinstall  requests\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_ner_bionlp13cg_md-0.5.1.tar.gz\n",
        "!pip install scispacy\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49OD4udTdCd6"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import logging\n",
        "import uuid\n",
        "import sys\n",
        "from Bio import Entrez, Medline\n",
        "from tqdm import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "import requests\n",
        "import spacy\n",
        "from Bio import Entrez, Medline\n",
        "import time\n",
        "import logging\n",
        "import csv\n",
        "import uuid\n",
        "import xml.etree.ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Python Script for Biomedical Literature Analysis: A Step-by-Step Explanation\n",
        "\n",
        "Imagine you're a detective trying to find connections between certain childhood inflammatory diseases (like MIS-C or Kawasaki Disease) and genetic factors, particularly in research authored by a specific group of scientists (like those led by Casanova). This script is your automated research assistant.\n",
        "\n",
        "**Overall Goal:**\n",
        "\n",
        "The script aims to:\n",
        "1.  Search the PubMed database (a vast library of biomedical research papers) for articles relevant to specific childhood inflammatory syndromes and genetics, with a potential focus on papers by certain authors.\n",
        "2.  Extract detailed information from these papers, including titles, authors, publication year, journal, and abstracts (summaries).\n",
        "3.  Identify gene names and genetic variants (like mutations or polymorphisms) mentioned within these papers, using both text analysis and links in NCBI (National Center for Biotechnology Information) databases.\n",
        "4.  Gather citation counts for these papers from both PubMed and Semantic Scholar (another academic search engine) to gauge their impact.\n",
        "5.  Consolidate all this information into a single, structured CSV (spreadsheet-like) file for further analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## The Pipeline: Step-by-Step Explanation\n",
        "\n",
        "### Phase 1: Setup and Configuration (The Blueprint)\n",
        "\n",
        "This is like gathering your tools and instructions before starting a big project.\n",
        "\n",
        "1.  **Defining Key Information (Lines 1-31 in the script):**\n",
        "    * `HGNC_TSV_URL`: The web address for a file from the HUGO Gene Nomenclature Committee (HGNC). This committee gives official names and symbols to human genes. This file is a master list.\n",
        "    * `APPROVED_GENE_SYMBOLS_FILE`: The name of a local file where the script will store a list of \"approved\" gene symbols extracted from the HGNC master list.\n",
        "    * `APPROVED_SYMBOL_COLUMN_NAME`, `STATUS_COLUMN_NAME`, `REQUIRED_STATUS`: These tell the script which columns in the HGNC file contain the gene symbol and its status (e.g., \"Approved\", \"Entry Withdrawn\"), and that we only want \"Approved\" ones.\n",
        "    * `APPROVED_GENE_SYMBOLS = set()`: An empty container (a set, which stores unique items) that will later be filled with the official gene symbols.\n",
        "    * `ENTREZ_EMAIL`, `ENTREZ_API_KEY`: Your \"credentials\" for using NCBI's Entrez system, which is the gateway to PubMed and other databases. The email is for NCBI to contact you if there's an issue, and the API key grants higher access limits.\n",
        "    * `MAX_RESULTS`, `BATCH_SIZE`, `RETRY_BATCH_SIZE`, `RETRIES`, `BASE_SLEEP`: Settings for how many results to fetch, how many papers to process in one go (a batch), how many to retry if a batch fails, how many times to retry, and how long to pause between requests (to be polite to the servers).\n",
        "    * `TARGET_AUTHOR`: A list of author names (and their variations) the script should pay special attention to.\n",
        "    * `PMIDS_TO_CHECK`, `CRITICAL_PMIDS`: Lists of specific PubMed IDs (PMIDs are unique identifiers for papers). `PMIDS_TO_CHECK` are papers to verify if they are found by the search query. `CRITICAL_PMIDS` are papers that *must* be included in the analysis, even if the main search doesn't find them.\n",
        "    * `_GENE_BLACKLIST_TERMS`, `GENE_BLACKLIST`: A list of words (like \"COVID\", \"RESULTS\", \"GENE\") that might look like gene names but aren't, or are too general. The script will ignore these if it finds them during text analysis.\n",
        "    * `VARIANT_KEYWORDS`: Words that signal the presence of a genetic variant discussion (e.g., \"mutation\", \"allele\").\n",
        "    * `QUERY`: This is the heart of the literature search. It's a complex string formatted for PubMed, telling it to find papers that:\n",
        "        * Mention \"Multisystem Inflammatory Syndrome in Children\" (MIS-C), \"PIMS\", \"Kawasaki Disease\" (either as official keywords or in the title/abstract).\n",
        "        * **AND** also mention terms related to genetics like \"Genetic Predisposition\", \"Mutation\", \"genes\", \"variants\", \"GWAS\" (Genome-Wide Association Study), etc., OR are authored by anyone in `TARGET_AUTHOR`.\n",
        "    * `logging.basicConfig(...)`: Sets up a log file (`pubmed_errors.log`) to record progress and any errors encountered.\n",
        "    * `Entrez.email = ...`, `Entrez.api_key = ...`: Tells the `Bio.Entrez` library (used for NCBI access) your email and API key.\n",
        "    * `nlp = spacy.load(...)`: Attempts to load a specialized AI model from \"SciSpaCy\". This model is trained to understand biomedical text and can identify things like gene names within sentences. If it fails to load, the script will still run but won't be as good at finding genes directly from text.\n",
        "\n",
        "### Phase 2: Preparing the Gene List (The Reference Manual)\n",
        "\n",
        "2.  **`create_approved_gene_list_from_hgnc` function (Lines 40-106):**\n",
        "    * **Purpose:** To create a clean list of officially approved human gene symbols.\n",
        "    * **How it works:**\n",
        "        * Downloads the `hgnc_complete_set.txt` file from the `HGNC_TSV_URL`.\n",
        "        * Reads this file (it's a tab-separated value file, like a spreadsheet).\n",
        "        * Finds the columns for gene symbols and their status.\n",
        "        * Goes row by row, and if a gene's status is \"Approved\", it adds the gene symbol (converted to uppercase) to a set.\n",
        "        * Saves this set of approved gene symbols into the `APPROVED_GENE_SYMBOLS_FILE` (`hgnc_approved_genes.txt`).\n",
        "    * **Why it's important:** When the script later tries to identify genes from paper texts, it can check them against this official list to reduce errors and ensure it's dealing with valid gene symbols.\n",
        "\n",
        "### Phase 3: Searching and Gathering Paper Data (The Investigation)\n",
        "\n",
        "3.  **`check_paper_in_query` function (Lines 108-119):**\n",
        "    * **Purpose:** To verify if a specific paper (given by its `pmid`) would be found by the main `QUERY`.\n",
        "    * **How it works:** Performs a mini-search using the `QUERY` and checks if the `pmid` is in the list of results.\n",
        "    * **Why it's important:** Useful for debugging the `QUERY` or ensuring critical papers are indeed covered.\n",
        "\n",
        "4.  **`search_pubmed` function (Lines 121-136):**\n",
        "    * **Purpose:** To execute the main search `QUERY` on PubMed and get a list of relevant paper IDs.\n",
        "    * **How it works:** Uses `Entrez.esearch` to send the query to PubMed and retrieves up to `MAX_RESULTS` PMIDs. It includes retries in case of network issues.\n",
        "    * **Output:** A list of PMIDs.\n",
        "\n",
        "5.  **`fetch_paper_titles` function (Lines 138-215):**\n",
        "    * **Purpose:** For a given list of PMIDs, this function fetches detailed information: title, authors, abstract, publication year, and journal. Crucially, it also performs text analysis on the title and abstract to find potential gene and variant mentions.\n",
        "    * **How it works:**\n",
        "        * Processes PMIDs in batches (`b_size`) for efficiency.\n",
        "        * Uses `Entrez.efetch` to get the full records for each paper in Medline format.\n",
        "        * Parses each record to extract:\n",
        "            * PMID, Title (`TI`), Authors (`AU`), Abstract (`AB`), Publication Date (`DP` to get the year), Journal (`JT` or `TA`).\n",
        "            * Checks if any of the `TARGET_AUTHOR`s are in the author list.\n",
        "        * **SciSpaCy Text Mining (if `nlp` model is loaded):**\n",
        "            * Combines the title and abstract into one block of text.\n",
        "            * Feeds this text to the SciSpaCy model (`doc = nlp(text_content)`).\n",
        "            * The model identifies entities labeled as \"GENE_OR_GENE_PRODUCT\".\n",
        "            * These candidate genes are filtered: must be longer than 2 characters, not just numbers, and not in the `GENE_BLACKLIST`.\n",
        "            * If the `APPROVED_GENE_SYMBOLS` list is available, it further filters these candidates to keep only those present in the approved list.\n",
        "            * It also looks for keywords from `VARIANT_KEYWORDS` and then checks nearby words for patterns that look like variant IDs (e.g., `rs12345`, `p.Gly12Val`).\n",
        "        * Stores all this information for each PMID in a dictionary (`paper_info`).\n",
        "    * **Output:** A dictionary where keys are PMIDs and values are dictionaries containing all the extracted details, including lists of `text_genes`, `text_variants`, and `text_variant_context`.\n",
        "\n",
        "6.  **`fetch_citation_counts` function (Lines 217-244):**\n",
        "    * **Purpose:** To find out how many times each paper (PMID) has been cited by other papers within the PubMed database.\n",
        "    * **How it works:**\n",
        "        * Uses `Entrez.elink` with `linkname=\"pubmed_pubmed_citedin\"`. This asks NCBI: \"For these PMIDs, which other PubMed papers cite them?\"\n",
        "        * Counts the number of citing papers for each input PMID.\n",
        "    * **Output:** A dictionary mapping PMIDs to their PubMed citation counts.\n",
        "\n",
        "7.  **`fetch_s2_citation_counts` function (Lines 246-294):**\n",
        "    * **Purpose:** To get citation counts from Semantic Scholar, an alternative academic search engine. This can provide a different or more up-to-date count.\n",
        "    * **How it works:**\n",
        "        * Uses the Semantic Scholar API (`https://api.semanticscholar.org/graph/v1/paper/batch`).\n",
        "        * Sends batches of PMIDs (prefixed with \"PMID:\") to the API.\n",
        "        * Parses the JSON response to get the `citationCount` for each paper.\n",
        "        * Includes specific handling for rate limits (API telling the script to slow down).\n",
        "    * **Output:** A dictionary mapping PMIDs to their Semantic Scholar citation counts.\n",
        "\n",
        "8.  **`fetch_linked_data` function (Lines 296-323):**\n",
        "    * **Purpose:** To find official links between papers (PMIDs) and entries in other NCBI databases, specifically the Gene database or the SNP (Single Nucleotide Polymorphism, a type of variant) database.\n",
        "    * **How it works:**\n",
        "        * Similar to `fetch_citation_counts`, uses `Entrez.elink`.\n",
        "        * For genes: `db=\"gene\"`, `linkname=\"pubmed_gene\"`. This asks: \"Which NCBI Gene IDs are linked to these PMIDs?\"\n",
        "        * For variants: `db=\"snp\"`, `linkname=\"pubmed_snp\"`. This asks: \"Which NCBI SNP IDs are linked to these PMIDs?\"\n",
        "    * **Output:** A dictionary mapping PMIDs to a list of linked Gene IDs or SNP IDs.\n",
        "\n",
        "9.  **`fetch_gene_names` function (Lines 326-349):**\n",
        "    * **Purpose:** NCBI Gene IDs are numbers (e.g., `7157`). This function converts these IDs into their human-readable official symbols (e.g., `TP53`).\n",
        "    * **How it works:**\n",
        "        * Takes a list of Gene IDs.\n",
        "        * Uses `Entrez.efetch` with `db=\"gene\"` to get detailed records for these Gene IDs.\n",
        "        * Parses the XML response to find the `Gene-ref_locus` (official symbol) for each ID.\n",
        "    * **Output:** A dictionary mapping Gene IDs to their symbols.\n",
        "\n",
        "10. **`fetch_variant_details` function (Lines 351-401):**\n",
        "    * **Purpose:** NCBI SNP IDs are also numbers. This function fetches more details for these variant IDs, such as their common `rs` number (e.g., `rs12345`), the different versions (alleles, like A, T, C, or G), and potentially the gene they are associated with.\n",
        "    * **How it works:**\n",
        "        * Takes a list of SNP IDs.\n",
        "        * Uses `Entrez.efetch` with `db=\"snp\"` to get detailed records.\n",
        "        * Parses the XML response to extract:\n",
        "            * `SNP_ID` (to form the `rs` number).\n",
        "            * Allele information from `GLOBAL_MAFS` (Minor Allele Frequencies).\n",
        "            * Associated gene name, if available.\n",
        "    * **Output:** A dictionary mapping SNP IDs to details like `rsid`, `alleles`, and `gene`.\n",
        "\n",
        "### Phase 4: Execution and Reporting (Putting It All Together)\n",
        "\n",
        "This happens inside the `if __name__ == \"__main__\":` block (Lines 403-550). This is the main control flow of the script.\n",
        "\n",
        "1.  **Initial Checks & Setup (Lines 404-425):**\n",
        "    * Ensures `ENTREZ_EMAIL` and `QUERY` are set.\n",
        "    * Warns if the SciSpaCy model (`nlp`) isn't loaded.\n",
        "    * **Load/Create Approved Gene List:**\n",
        "        * Checks if `APPROVED_GENE_SYMBOLS_FILE` exists.\n",
        "        * If not, calls `create_approved_gene_list_from_hgnc` to create it.\n",
        "        * If it exists (or was just created), it reads the gene symbols from this file into the `APPROVED_GENE_SYMBOLS` set for later use in validation.\n",
        "\n",
        "2.  **Running the Search and Fetching Data (Lines 427-466):**\n",
        "    * `check_paper_in_query`: Calls the function for each PMID in `PMIDS_TO_CHECK`.\n",
        "    * `paper_ids_list = search_pubmed()`: Performs the main PubMed search.\n",
        "    * **Add Critical PMIDs:** Ensures all PMIDs from `CRITICAL_PMIDS` are included in `paper_ids_list`, even if the search didn't find them.\n",
        "    * `paper_info_dict, ... = fetch_paper_titles(...)`: Gets titles, abstracts, and performs SciSpaCy text mining for all identified papers. Retries failed ones.\n",
        "    * `citation_counts_dict, ... = fetch_citation_counts(...)`: Gets PubMed citation counts. Retries.\n",
        "    * `s2_citation_counts_dict = fetch_s2_citation_counts(...)`: Gets Semantic Scholar citation counts.\n",
        "    * `paper_to_ncbi_genes, ... = fetch_linked_data(..., \"gene\", ...)`: Finds NCBI Gene IDs linked to the papers.\n",
        "    * `paper_to_ncbi_variants, ... = fetch_linked_data(..., \"snp\", ...)`: Finds NCBI SNP IDs linked to the papers.\n",
        "    * `gene_names_dict, ... = fetch_gene_names(...)`: Converts all unique linked Gene IDs to gene symbols.\n",
        "    * `variant_details_dict, ... = fetch_variant_details(...)`: Gets details for all unique linked SNP IDs.\n",
        "\n",
        "3.  **Structuring and Consolidating Results (Lines 468-530):**\n",
        "    * This is a crucial loop where all the gathered information is combined for each paper.\n",
        "    * It iterates through each paper in `paper_info_dict`.\n",
        "    * For each paper (PMID):\n",
        "        * It retrieves the basic info (title, year, journal, author flag).\n",
        "        * It gets the text-mined genes/variants from `paper_info_dict`.\n",
        "        * It retrieves the linked NCBI gene symbols (using `gene_names_dict`) and linked variant details (using `variant_details_dict`).\n",
        "        * It gets the PubMed and Semantic Scholar citation counts.\n",
        "        * **Logic for Output Rows:**\n",
        "            * If a paper has NCBI-linked genes, a row is created for each gene.\n",
        "            * If a paper has NCBI-linked variants, a row is created for each variant (showing its associated gene if known, or a placeholder).\n",
        "            * If a paper has genes identified by SciSpaCy *that were not already covered by NCBI links*, a row is created for these text-mined genes, potentially associating them with text-mined variants if found.\n",
        "            * If a paper has *no* specific gene or variant data found (neither linked nor from text), a placeholder row is created for that paper to indicate it was retrieved by the search but had no identifiable genetic information, or that SciSpaCy hits were filtered out.\n",
        "        * Each such piece of information (PMID, Title, Year, Gene, Variant, Citations, etc.) is added as a dictionary to a list called `results_list`. One paper might lead to multiple rows in `results_list` if it discusses multiple genes/variants.\n",
        "\n",
        "4.  **Saving the Output (Lines 532-546):**\n",
        "    * `results_list.sort(...)`: Sorts all the collected rows, primarily by Semantic Scholar citation count (highest first), then by PubMed citation count, then by PMID.\n",
        "    * Checks if `results_list` actually contains any data.\n",
        "    * If yes, it defines the column headers (`fieldnames`) for the output CSV file.\n",
        "    * It creates a unique filename for the CSV (e.g., `pubmed_genetic_results_abcdef12.csv`).\n",
        "    * Writes all the dictionaries in `results_list` as rows into this CSV file.\n",
        "    * Prints a message indicating where the results were saved.\n",
        "\n",
        "5.  **Script End (Lines 548-550):**\n",
        "    * Prints \"Script finished.\" to the console and to the log file.\n",
        "\n",
        "---\n",
        "\n",
        "This detailed process allows the script to systematically gather, process, and organize a wealth of information from biomedical literature, making it easier for researchers to identify trends, key papers, and genetic factors related to their area of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfcXu8b0ffmg",
        "outputId": "6dbf029d-366e-4570-9df4-f0966b056f15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_ner_bionlp13cg_md' (0.5.1) was trained with spaCy v3.4.1 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'hgnc_approved_genes.txt' not found. Attempting to download and create it...\n",
            "Attempting to download HGNC gene data from: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Error downloading HGNC data: 404 Client Error: Not Found for url: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n",
            "ERROR:root:Failed to create 'hgnc_approved_genes.txt'.\n",
            "WARNING:root:Approved gene symbols file ('hgnc_approved_genes.txt') not found. Gene validation will be limited.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error downloading HGNC data: 404 Client Error: Not Found for url: https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\n",
            "Failed to create 'hgnc_approved_genes.txt'. Gene validation against HGNC list will be skipped.\n",
            "Warning: 'hgnc_approved_genes.txt' not found. SciSpaCy gene validation against HGNC list will be skipped.\n",
            "Paper 33106546 found in query.\n",
            "Searching PubMed...\n",
            "Found 923 papers initially from PubMed search.\n",
            "Fetching paper titles, authors, abstracts, year, and journal...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching title batches:   0%|          | 0/93 [00:00<?, ?batch/s]WARNING:root:Approved gene list (APPROVED_GENE_SYMBOLS) is empty. Keeping SciSpaCy genes without HGNC validation.\n",
            "Fetching title batches: 100%|██████████| 93/93 [01:47<00:00,  1.15s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched details for 923 papers.\n",
            "Fetching citation counts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching citation batches: 100%|██████████| 93/93 [00:45<00:00,  2.03batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching citation counts from Semantic Scholar...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching S2 citation batches:  23%|██▎       | 42/185 [03:12<10:54,  4.58s/batch]ERROR:root:Semantic Scholar API request timed out for batch starting with PMID:35571208, attempt 1.\n",
            "Fetching S2 citation batches:  71%|███████▏  | 132/185 [10:15<04:14,  4.81s/batch]ERROR:root:Semantic Scholar API request failed for batch PMID:26472029, attempt 1: 504 Server Error: Gateway Timeout for url: https://api.semanticscholar.org/graph/v1/paper/batch?fields=citationCount,externalIds\n",
            "Fetching S2 citation batches: 100%|██████████| 185/185 [14:21<00:00,  4.66s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving linked NCBI genes in batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching NCBI genes batches: 100%|██████████| 93/93 [00:36<00:00,  2.54batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving linked NCBI variants in batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching NCBI variants batches: 100%|██████████| 93/93 [00:35<00:00,  2.59batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique NCBI-linked gene IDs to fetch names for: 1560\n",
            "Fetching gene names...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing gene name batches: 100%|██████████| 156/156 [04:42<00:00,  1.81s/batch]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique NCBI-linked variant IDs to fetch details for: 582\n",
            "Fetching variant details...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing variant detail batches: 100%|██████████| 59/59 [00:36<00:00,  1.61batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing and structuring results...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Structuring results: 100%|██████████| 923/923 [00:00<00:00, 52871.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results saved to pubmed_genetic_results_68a3f3d2.csv\n",
            "Script finished.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "HGNC_TSV_URL = \"https://storage.googleapis.com/public-download-files/hgnc/tsv/hgnc_complete_set.txt\"\n",
        "APPROVED_GENE_SYMBOLS_FILE = \"hgnc_approved_genes.txt\"\n",
        "APPROVED_SYMBOL_COLUMN_NAME = \"symbol\"\n",
        "STATUS_COLUMN_NAME = \"status\"\n",
        "REQUIRED_STATUS = \"Approved\"\n",
        "APPROVED_GENE_SYMBOLS = set()\n",
        "\n",
        "ENTREZ_EMAIL = \"michal.uppal@gmail.com\"\n",
        "ENTREZ_API_KEY = \"be8c41829a4406a6d8a0f2d8ec242d9db108\"\n",
        "MAX_RESULTS = 5000\n",
        "BATCH_SIZE = 10\n",
        "RETRY_BATCH_SIZE = 3\n",
        "RETRIES = 3\n",
        "BASE_SLEEP = 0.1\n",
        "TARGET_AUTHOR = ['Casanova', 'Jean-Laurent Casanova', 'Casanova JL', 'Casanova J-L']\n",
        "PMIDS_TO_CHECK = ['33106546']\n",
        "CRITICAL_PMIDS = ['33106546']\n",
        "\n",
        "_GENE_BLACKLIST_TERMS = {\n",
        "    'SARS', 'MIS-C', 'PIMS', 'PIMS-TS', 'COVID', 'HLA', 'HIV', 'EBV',\n",
        "    'CONCLUSIONS', 'BACKGROUND', 'PATIENTS', 'METHODS', 'RESULTS', 'AND',\n",
        "    'SD', 'INTRODUCTION', 'ABSTRACT', 'DISCUSSION', 'GENE', 'VARIANT'\n",
        "}\n",
        "GENE_BLACKLIST = {term.upper() for term in _GENE_BLACKLIST_TERMS}\n",
        "\n",
        "VARIANT_KEYWORDS = {'variant', 'variants', 'mutation', 'mutations', 'allele', 'alleles'}\n",
        "QUERY = (\n",
        "    \"(Multisystem Inflammatory Syndrome in Children[MeSH Terms] OR \"\n",
        "    \"MIS-C[Title/Abstract] OR PIMS[Title/Abstract] OR PIMS-TS[Title/Abstract] OR \"\n",
        "    \"Paediatric Inflammatory Multisystem Syndrome[Title/Abstract] OR \"\n",
        "    \"Kawasaki Disease[MeSH Terms] OR Kawasaki Disease[Title/Abstract] OR Kawasaki Syndrome[Title/Abstract]) AND \"\n",
        "    \"(Genetic Predisposition to Disease[MeSH Terms] OR Polymorphism, Genetic[MeSH Terms] OR \"\n",
        "    \"Mutation[MeSH Terms] OR Genome-Wide Association Study[MeSH Terms] OR \"\n",
        "    \"genes[Title/Abstract] OR variants[Title/Abstract] OR genetic association[Title/Abstract] OR \"\n",
        "    \"polymorphism[Title/Abstract] OR mutation[Title/Abstract] OR gwas[Title/Abstract] OR \"\n",
        "    \"inborn errors[Title/Abstract] OR genetic defects[Title/Abstract] OR \"\n",
        "    \"genomics[Title/Abstract] OR gene expression[Title/Abstract] OR \"\n",
        "    f\"({' OR '.join(f'{author}[Author]' for author in TARGET_AUTHOR)}))\"\n",
        ")\n",
        "\n",
        "logging.basicConfig(filename='pubmed_errors.log', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "Entrez.email = ENTREZ_EMAIL\n",
        "if ENTREZ_API_KEY:\n",
        "    Entrez.api_key = ENTREZ_API_KEY\n",
        "\n",
        "nlp = None\n",
        "try:\n",
        "    nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
        "    logging.info(\"SciSpaCy model 'en_ner_bionlp13cg_md' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to load SciSpaCy model 'en_ner_bionlp13cg_md': {e}.\")\n",
        "    print(f\"CRITICAL: Failed to load SciSpaCy model. Text processing for genes/variants will be skipped. Error: {e}\")\n",
        "\n",
        "def create_approved_gene_list_from_hgnc(url, output_file, symbol_column_name, status_column_name, required_status):\n",
        "    print(f\"Attempting to download HGNC gene data from: {url}\")\n",
        "    logging.info(f\"Attempting to download HGNC gene data from: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        print(\"HGNC download successful.\")\n",
        "        logging.info(\"HGNC download successful.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading HGNC data: {e}\")\n",
        "        logging.error(f\"Error downloading HGNC data: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Processing HGNC TSV data to extract '{symbol_column_name}' where '{status_column_name}' is '{required_status}'...\")\n",
        "    lines = response.text.splitlines()\n",
        "    reader = csv.reader(lines, delimiter='\\t')\n",
        "\n",
        "    extracted_symbols = set()\n",
        "    header = None\n",
        "    symbol_column_index = -1\n",
        "    status_column_index = -1\n",
        "\n",
        "    try:\n",
        "        header = next(reader)\n",
        "\n",
        "        if symbol_column_name in header:\n",
        "            symbol_column_index = header.index(symbol_column_name)\n",
        "            logging.info(f\"Found '{symbol_column_name}' column at index {symbol_column_index} in HGNC data.\")\n",
        "        else:\n",
        "            print(f\"Error: Column '{symbol_column_name}' not found in HGNC header: {header}\")\n",
        "            logging.error(f\"Error: Column '{symbol_column_name}' not found in HGNC header: {header}\")\n",
        "            return False\n",
        "\n",
        "        if status_column_name in header:\n",
        "            status_column_index = header.index(status_column_name)\n",
        "            logging.info(f\"Found '{status_column_name}' column at index {status_column_index} in HGNC data.\")\n",
        "        else:\n",
        "            print(f\"Error: Column '{status_column_name}' not found in HGNC header: {header}\")\n",
        "            logging.error(f\"Error: Column '{status_column_name}' not found in HGNC header: {header}\")\n",
        "            return False\n",
        "\n",
        "        for row_number, row in enumerate(reader, 1):\n",
        "            if not row or all(not cell for cell in row):\n",
        "                logging.debug(f\"Skipping empty or blank row {row_number + 1} in HGNC data.\")\n",
        "                continue\n",
        "\n",
        "            if len(row) > symbol_column_index and len(row) > status_column_index:\n",
        "                current_status = row[status_column_index].strip()\n",
        "                if current_status == required_status:\n",
        "                    symbol = row[symbol_column_index].strip()\n",
        "                    if symbol:\n",
        "                        extracted_symbols.add(symbol.upper())\n",
        "                    else:\n",
        "                        logging.debug(f\"HGNC data: Row {row_number + 1} has status '{required_status}' but symbol is empty.\")\n",
        "            else:\n",
        "                logging.warning(f\"HGNC data: Row {row_number + 1} has insufficient columns. Length: {len(row)}, Row content (first 5 cells): {row[:5]}\")\n",
        "\n",
        "    except StopIteration:\n",
        "        print(\"Error: HGNC file appears to be empty after header or header is missing.\")\n",
        "        logging.error(\"Error: HGNC file appears to be empty after header or header is missing.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the HGNC TSV file: {e}\")\n",
        "        logging.error(f\"An error occurred while processing the HGNC TSV file: {e}\")\n",
        "        return False\n",
        "\n",
        "    if not extracted_symbols:\n",
        "        print(f\"No gene symbols with status '{required_status}' were extracted from HGNC data. Please check the file content and specified column names/status.\")\n",
        "        logging.warning(f\"No gene symbols with status '{required_status}' were extracted from HGNC data.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Extracted {len(extracted_symbols)} unique gene symbols with status '{required_status}'.\")\n",
        "    logging.info(f\"Extracted {len(extracted_symbols)} unique gene symbols with status '{required_status}'.\")\n",
        "\n",
        "    try:\n",
        "        with open(output_file, 'w') as f:\n",
        "            for symbol in sorted(list(extracted_symbols)):\n",
        "                f.write(symbol + \"\\n\")\n",
        "        print(f\"Successfully saved approved gene symbols to {output_file}\")\n",
        "        logging.info(f\"Successfully saved approved gene symbols to {output_file}\")\n",
        "        return True\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing HGNC gene symbols to output file {output_file}: {e}\")\n",
        "        logging.error(f\"Error writing HGNC gene symbols to output file {output_file}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_paper_in_query(pmid, query_to_check=QUERY):\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"pubmed\", term=query_to_check, retmax=10000)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "        is_found = pmid in record['IdList']\n",
        "        logging.info(f\"Paper {pmid} {'found' if is_found else 'not found'} in query.\")\n",
        "        print(f\"Paper {pmid} {'found' if is_found else 'not found'} in query.\")\n",
        "        return is_found\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking if paper {pmid} is in query: {e}\")\n",
        "        logging.error(f\"Error checking if paper {pmid} is in query: {e}\")\n",
        "        return False\n",
        "\n",
        "def search_pubmed(query_to_search=QUERY, max_r=MAX_RESULTS, num_retries=RETRIES):\n",
        "    print(\"Searching PubMed...\")\n",
        "    logging.info(f\"Searching PubMed with query (first 200 chars): {query_to_search[:200]}...\")\n",
        "    for attempt in range(num_retries):\n",
        "        try:\n",
        "            handle = Entrez.esearch(db=\"pubmed\", term=query_to_search, retmax=max_r)\n",
        "            record = Entrez.read(handle)\n",
        "            handle.close()\n",
        "            paper_ids = record['IdList']\n",
        "            logging.info(f\"Found {len(paper_ids)} papers in PubMed: {paper_ids[:10]}...\")\n",
        "            return paper_ids\n",
        "        except Exception as e:\n",
        "            logging.error(f\"PubMed search attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == num_retries - 1:\n",
        "                print(f\"Error searching PubMed after {num_retries} attempts: {e}\")\n",
        "            time.sleep(1 + attempt)\n",
        "    return []\n",
        "\n",
        "def fetch_paper_titles(paper_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    global APPROVED_GENE_SYMBOLS\n",
        "    paper_info = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching paper titles, authors, abstracts, year, and journal...\")\n",
        "    if not nlp:\n",
        "        logging.error(\"SciSpaCy model (nlp) not loaded. Gene/variant extraction from text will be skipped.\")\n",
        "\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=\"Fetching title batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(batch), rettype=\"medline\", retmode=\"text\")\n",
        "                records = list(Medline.parse(handle))\n",
        "                handle.close()\n",
        "                for record in records:\n",
        "                    if \"PMID\" in record and \"TI\" in record:\n",
        "                        authors = record.get(\"AU\", [])\n",
        "                        abstract = record.get(\"AB\", \"\")\n",
        "                        has_target_author = any(\n",
        "                            any(target.lower() in author.lower() for target in TARGET_AUTHOR)\n",
        "                            for author in authors\n",
        "                        ) if TARGET_AUTHOR else False\n",
        "\n",
        "                        dp_string = record.get(\"DP\", \"\")\n",
        "                        year = \"N/A\"\n",
        "                        if dp_string:\n",
        "                            match = re.search(r'\\b(\\d{4})\\b', dp_string)\n",
        "                            if match:\n",
        "                                year = match.group(1)\n",
        "\n",
        "                        journal = record.get(\"JT\", record.get(\"TA\", \"N/A\"))\n",
        "\n",
        "                        text_content = f\"{record.get('TI', '')} {abstract}\"\n",
        "\n",
        "                        validated_text_genes = []\n",
        "                        text_variants = []\n",
        "                        text_variant_context = []\n",
        "\n",
        "                        if nlp:\n",
        "                            doc = nlp(text_content)\n",
        "                            scispacy_genes = [ent.text for ent in doc.ents if ent.label_ == \"GENE_OR_GENE_PRODUCT\"]\n",
        "                            current_paper_validated_genes = set()\n",
        "\n",
        "                            for gene_candidate_original_case in scispacy_genes:\n",
        "                                gene_candidate_upper = gene_candidate_original_case.upper()\n",
        "                                if not (len(gene_candidate_original_case) > 2 and \\\n",
        "                                        not gene_candidate_original_case.isdigit() and \\\n",
        "                                        gene_candidate_upper not in GENE_BLACKLIST):\n",
        "                                    logging.debug(f\"Paper {record['PMID']}: Gene '{gene_candidate_original_case}' failed pre-filtering.\")\n",
        "                                    continue\n",
        "\n",
        "                                if APPROVED_GENE_SYMBOLS:\n",
        "                                    if gene_candidate_upper not in APPROVED_GENE_SYMBOLS:\n",
        "                                        logging.info(f\"Paper {record['PMID']}: SciSpaCy gene '{gene_candidate_original_case}' NOT in approved symbols. Discarding.\")\n",
        "                                        continue\n",
        "                                    current_paper_validated_genes.add(gene_candidate_original_case)\n",
        "                                else:\n",
        "                                    current_paper_validated_genes.add(gene_candidate_original_case)\n",
        "                                    if i == 0 and attempt == 0 and not getattr(fetch_paper_titles, 'warned_no_approved_list', False):\n",
        "                                        logging.warning(f\"Approved gene list (APPROVED_GENE_SYMBOLS) is empty. Keeping SciSpaCy genes without HGNC validation.\")\n",
        "                                        fetch_paper_titles.warned_no_approved_list = True\n",
        "\n",
        "                            validated_text_genes = sorted(list(current_paper_validated_genes))\n",
        "\n",
        "                            for token in doc:\n",
        "                                if token.text.lower() in VARIANT_KEYWORDS:\n",
        "                                    start = max(0, token.i - 5)\n",
        "                                    end = min(len(doc), token.i + 6)\n",
        "                                    window = doc[start:end]\n",
        "                                    variant_found = False\n",
        "                                    for t_variant in window:\n",
        "                                        if (t_variant.text.startswith('rs') and t_variant.text[2:].isdigit()) or \\\n",
        "                                           (t_variant.text.startswith('p.') and any(c.isdigit() for c in t_variant.text)):\n",
        "                                            text_variants.append(t_variant.text)\n",
        "                                            variant_found = True\n",
        "                                            break\n",
        "                                    if not variant_found:\n",
        "                                        text_variant_context.append(token.text)\n",
        "\n",
        "                            text_variants = sorted(list(set(v for v in text_variants if len(v) > 3 or v.startswith('rs'))))\n",
        "                            text_variant_context = sorted(list(set(c for c in text_variant_context if c in VARIANT_KEYWORDS)))\n",
        "\n",
        "                        paper_info[record[\"PMID\"]] = {\n",
        "                            \"title\": record[\"TI\"],\n",
        "                            \"authors\": authors,\n",
        "                            \"year\": year,\n",
        "                            \"journal\": journal,\n",
        "                            \"has_target_author\": has_target_author,\n",
        "                            \"abstract\": abstract,\n",
        "                            \"text_genes\": validated_text_genes,\n",
        "                            \"text_variants\": text_variants,\n",
        "                            \"text_variant_context\": text_variant_context\n",
        "                        }\n",
        "                        logging.debug(f\"Paper {record['PMID']} metadata: title='{record['TI']}', year='{year}', journal='{journal}', validated_text_genes={validated_text_genes}, text_variants={text_variants}\")\n",
        "                    else:\n",
        "                        logging.warning(f\"Skipping record in batch {i // b_size + 1}: Missing PMID or Title. Record PMID: {record.get('PMID', 'N/A')}\")\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch titles batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching title batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return paper_info, failed_ids\n",
        "\n",
        "def fetch_citation_counts(paper_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    citation_counts = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching citation counts...\")\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=\"Fetching citation batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.elink(dbfrom=\"pubmed\", db=\"pubmed\", linkname=\"pubmed_pubmed_citedin\", id=\",\".join(batch))\n",
        "                record = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for linkset in record:\n",
        "                    paper_id = linkset[\"IdList\"][0]\n",
        "                    try:\n",
        "                        if \"LinkSetDb\" in linkset and linkset[\"LinkSetDb\"]:\n",
        "                            citation_ids = [link[\"Id\"] for link in linkset[\"LinkSetDb\"][0][\"Link\"]]\n",
        "                            citation_counts[paper_id] = len(citation_ids)\n",
        "                        else:\n",
        "                            citation_counts[paper_id] = 0\n",
        "                    except Exception as e_inner:\n",
        "                        logging.error(f\"Error processing citation linkset for paper {paper_id}: {e_inner}\")\n",
        "                        citation_counts[paper_id] = 0\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch citations batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching citation batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return citation_counts, failed_ids\n",
        "\n",
        "def fetch_s2_citation_counts(pmids, s2_batch_size=5, num_retries=RETRIES):\n",
        "    s2_citation_counts = {}\n",
        "    print(\"Fetching citation counts from Semantic Scholar...\")\n",
        "    logging.info(f\"Starting Semantic Scholar citation fetch for {len(pmids)} PMIDs.\")\n",
        "\n",
        "    s2_api_url = \"https://api.semanticscholar.org/graph/v1/paper/batch?fields=citationCount,externalIds\"\n",
        "\n",
        "    pmids_with_prefix = [f\"PMID:{pmid}\" for pmid in pmids]\n",
        "\n",
        "    for i in tqdm(range(0, len(pmids_with_prefix), s2_batch_size), desc=\"Fetching S2 citation batches\", unit=\"batch\"):\n",
        "        batch_ids = pmids_with_prefix[i:i + s2_batch_size]\n",
        "        original_pmids_in_batch = pmids[i:i + s2_batch_size]\n",
        "\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                response = requests.post(s2_api_url, json={\"ids\": batch_ids}, timeout=20)\n",
        "\n",
        "                if response.status_code == 429:\n",
        "                    retry_after = response.headers.get(\"Retry-After\", \"60\")\n",
        "                    print(f\"Semantic Scholar rate limit hit. Retrying after {retry_after} seconds...\")\n",
        "                    logging.warning(f\"Semantic Scholar rate limit hit. Retrying after {retry_after} seconds for batch starting with {batch_ids[0]}\")\n",
        "                    time.sleep(int(retry_after) + 5)\n",
        "                    continue\n",
        "\n",
        "                response.raise_for_status()\n",
        "                data_batch = response.json()\n",
        "\n",
        "                for idx, paper_data in enumerate(data_batch):\n",
        "                    original_pmid = original_pmids_in_batch[idx]\n",
        "                    if paper_data:\n",
        "                        s2_citation_counts[original_pmid] = paper_data.get(\"citationCount\", 0)\n",
        "                    else:\n",
        "                        s2_citation_counts[original_pmid] = 0\n",
        "                        logging.info(f\"PMID {original_pmid} not found or no data in Semantic Scholar batch response.\")\n",
        "\n",
        "                time.sleep(3.5)\n",
        "                break\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                logging.error(f\"Semantic Scholar API request timed out for batch starting with {batch_ids[0]}, attempt {attempt + 1}.\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Semantic Scholar API request timed out after {num_retries} attempts for batch {batch_ids[0]}. Skipping this batch.\")\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                logging.error(f\"Semantic Scholar API request failed for batch {batch_ids[0]}, attempt {attempt + 1}: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Semantic Scholar API request failed after {num_retries} attempts for batch {batch_ids[0]}. Skipping batch.\")\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "            except Exception as e_general:\n",
        "                logging.error(f\"Unexpected error processing Semantic Scholar batch {batch_ids[0]}, attempt {attempt + 1}: {e_general}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    for pmid_in_batch in original_pmids_in_batch: s2_citation_counts[pmid_in_batch] = 0\n",
        "                time.sleep(5 * (attempt + 1))\n",
        "\n",
        "    logging.info(f\"Finished Semantic Scholar citation fetch. Found counts for {len(s2_citation_counts)} PMIDs.\")\n",
        "    return s2_citation_counts\n",
        "\n",
        "def fetch_linked_data(paper_ids, link_db, link_name, desc_name, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    paper_to_items = {}\n",
        "    failed_ids = []\n",
        "    print(f\"Retrieving linked {desc_name} in batches...\")\n",
        "    for i in tqdm(range(0, len(paper_ids), b_size), desc=f\"Fetching {desc_name} batches\", unit=\"batch\"):\n",
        "        batch = paper_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.elink(dbfrom=\"pubmed\", db=link_db, linkname=link_name, id=\",\".join(batch))\n",
        "                record = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for linkset in record:\n",
        "                    paper_id = linkset[\"IdList\"][0]\n",
        "                    try:\n",
        "                        if \"LinkSetDb\" in linkset and linkset[\"LinkSetDb\"]:\n",
        "                            item_ids = [link[\"Id\"] for link in linkset[\"LinkSetDb\"][0][\"Link\"]]\n",
        "                            paper_to_items[paper_id] = item_ids\n",
        "                        else:\n",
        "                            paper_to_items[paper_id] = []\n",
        "                    except Exception as e_inner:\n",
        "                        logging.error(f\"Error processing {desc_name} linkset for paper {paper_id}: {e_inner}\")\n",
        "                        paper_to_items[paper_id] = []\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch linked {desc_name} batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching {desc_name} batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return paper_to_items, failed_ids\n",
        "\n",
        "\n",
        "def fetch_gene_names(gene_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    gene_names_map = {}\n",
        "    failed_ids = []\n",
        "    print(\"Fetching gene names...\")\n",
        "    if not gene_ids: return {}, []\n",
        "    for i in tqdm(range(0, len(gene_ids), b_size), desc=\"Processing gene name batches\", unit=\"batch\"):\n",
        "        batch = gene_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"gene\", id=\",\".join(batch), retmode=\"xml\")\n",
        "                records = Entrez.read(handle)\n",
        "                handle.close()\n",
        "                for record_xml in records:\n",
        "                    gene_id = record_xml[\"Entrezgene_track-info\"][\"Gene-track\"][\"Gene-track_geneid\"]\n",
        "                    symbol = record_xml.get(\"Entrezgene_gene\", {}).get(\"Gene-ref\", {}).get(\"Gene-ref_locus\", \"Unknown Symbol\")\n",
        "                    gene_names_map[gene_id] = symbol\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch gene names batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching gene name batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return gene_names_map, failed_ids\n",
        "\n",
        "def fetch_variant_details(variant_ids, b_size=BATCH_SIZE, num_retries=RETRIES):\n",
        "    variant_details_map = {}\n",
        "    failed_ids = []\n",
        "    ns = {'default': 'https://www.ncbi.nlm.nih.gov/SNP/docsum'}\n",
        "    print(\"Fetching variant details...\")\n",
        "    if not variant_ids: return {}, []\n",
        "    for i in tqdm(range(0, len(variant_ids), b_size), desc=\"Processing variant detail batches\", unit=\"batch\"):\n",
        "        batch = variant_ids[i:i + b_size]\n",
        "        for attempt in range(num_retries):\n",
        "            try:\n",
        "                handle = Entrez.efetch(db=\"snp\", id=\",\".join(batch), retmode=\"xml\")\n",
        "                xml_data = handle.read().decode('utf-8')\n",
        "                handle.close()\n",
        "                if not xml_data.strip():\n",
        "                    logging.warning(f\"Empty XML response for variant batch {i // b_size + 1}\")\n",
        "                    break\n",
        "                root = ET.fromstring(xml_data)\n",
        "                for doc in root.findall(\".//default:DocumentSummary\", namespaces=ns):\n",
        "                    snp_id_element = doc.find(\"default:SNP_ID\", namespaces=ns)\n",
        "                    variant_id = snp_id_element.text if snp_id_element is not None else \"UnknownID\"\n",
        "                    rsid = f\"rs{variant_id}\" if variant_id != \"UnknownID\" else \"UnknownRSID\"\n",
        "\n",
        "                    alleles = \"N/A\"\n",
        "                    global_mafs = doc.find(\"default:GLOBAL_MAFS\", namespaces=ns)\n",
        "                    if global_mafs is not None:\n",
        "                        alleles_set = set()\n",
        "                        maf_elements = global_mafs.findall(\"default:MAF\", namespaces=ns)\n",
        "                        for maf in maf_elements:\n",
        "                            freq = maf.find(\"default:FREQ\", namespaces=ns)\n",
        "                            if freq is not None and freq.text:\n",
        "                                try:\n",
        "                                    allele_part = freq.text.split('=')[0]\n",
        "                                    alleles_set.add(allele_part)\n",
        "                                except IndexError:\n",
        "                                    logging.warning(f\"Malformed FREQ text for variant {rsid}: {freq.text}\")\n",
        "                        if alleles_set:\n",
        "                             alleles = ','.join(sorted(alleles_set))\n",
        "\n",
        "                    gene_name_from_variant = \"\"\n",
        "                    genes_element = doc.find(\"default:GENES\", namespaces=ns)\n",
        "                    if genes_element is not None:\n",
        "                        gene_e = genes_element.find(\"default:GENE_E\", namespaces=ns)\n",
        "                        if gene_e is not None:\n",
        "                            name_element = gene_e.find(\"default:NAME\", namespaces=ns)\n",
        "                            if name_element is not None and name_element.text:\n",
        "                                gene_name_from_variant = name_element.text\n",
        "                    variant_details_map[variant_id] = {\"rsid\": rsid, \"alleles\": alleles, \"gene\": gene_name_from_variant}\n",
        "                time.sleep(max(BASE_SLEEP, b_size / 50.0))\n",
        "                break\n",
        "            except ET.ParseError as e_parse:\n",
        "                logging.error(f\"XML ParseError for variant batch {i // b_size + 1}: {e_parse}. Data (first 500 chars): {xml_data[:500]}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Fetch variant details batch {i // b_size + 1} attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt == num_retries - 1:\n",
        "                    print(f\"Error fetching variant detail batch {i // b_size + 1} after {num_retries} attempts: {e}\")\n",
        "                    failed_ids.extend(batch)\n",
        "                time.sleep(1 + attempt)\n",
        "    return variant_details_map, failed_ids\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not ENTREZ_EMAIL:\n",
        "        raise ValueError(\"ENTREZ_EMAIL must be set\")\n",
        "    if not QUERY:\n",
        "        raise ValueError(\"QUERY must be set\")\n",
        "    if not nlp:\n",
        "        print(\"WARNING: SciSpaCy model not loaded. Text-based gene/variant extraction will be limited.\")\n",
        "\n",
        "    if not os.path.exists(APPROVED_GENE_SYMBOLS_FILE):\n",
        "        print(f\"'{APPROVED_GENE_SYMBOLS_FILE}' not found. Attempting to download and create it...\")\n",
        "        if not create_approved_gene_list_from_hgnc(HGNC_TSV_URL, APPROVED_GENE_SYMBOLS_FILE,\n",
        "                                                    APPROVED_SYMBOL_COLUMN_NAME, STATUS_COLUMN_NAME, REQUIRED_STATUS):\n",
        "            print(f\"Failed to create '{APPROVED_GENE_SYMBOLS_FILE}'. Gene validation against HGNC list will be skipped.\")\n",
        "            logging.error(f\"Failed to create '{APPROVED_GENE_SYMBOLS_FILE}'.\")\n",
        "        else:\n",
        "             print(f\"Successfully created '{APPROVED_GENE_SYMBOLS_FILE}'.\")\n",
        "\n",
        "    if os.path.exists(APPROVED_GENE_SYMBOLS_FILE):\n",
        "        try:\n",
        "            with open(APPROVED_GENE_SYMBOLS_FILE, 'r') as f:\n",
        "                APPROVED_GENE_SYMBOLS = {line.strip().upper() for line in f if line.strip()}\n",
        "            if APPROVED_GENE_SYMBOLS:\n",
        "                logging.info(f\"Successfully loaded {len(APPROVED_GENE_SYMBOLS)} approved gene symbols from {APPROVED_GENE_SYMBOLS_FILE}.\")\n",
        "                print(f\"Loaded {len(APPROVED_GENE_SYMBOLS)} approved gene symbols for validation.\")\n",
        "            else:\n",
        "                logging.warning(f\"No gene symbols loaded from {APPROVED_GENE_SYMBOLS_FILE} (file might be empty).\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading gene symbols from {APPROVED_GENE_SYMBOLS_FILE}: {e}\")\n",
        "            print(f\"Error loading gene symbols from {APPROVED_GENE_SYMBOLS_FILE}. Validation may be affected.\")\n",
        "    else:\n",
        "        logging.warning(f\"Approved gene symbols file ('{APPROVED_GENE_SYMBOLS_FILE}') not found. Gene validation will be limited.\")\n",
        "        print(f\"Warning: '{APPROVED_GENE_SYMBOLS_FILE}' not found. SciSpaCy gene validation against HGNC list will be skipped.\")\n",
        "\n",
        "    for pmid_to_check in PMIDS_TO_CHECK:\n",
        "        check_paper_in_query(pmid_to_check)\n",
        "\n",
        "    paper_ids_list = search_pubmed()\n",
        "    print(f\"Found {len(paper_ids_list)} papers initially from PubMed search.\")\n",
        "\n",
        "    initial_pmid_count = len(paper_ids_list)\n",
        "    paper_ids_set = set(paper_ids_list)\n",
        "    for critical_pmid in CRITICAL_PMIDS:\n",
        "        if critical_pmid not in paper_ids_set:\n",
        "            paper_ids_set.add(critical_pmid)\n",
        "            logging.info(f\"Manually added critical PMID {critical_pmid} to paper_ids_set\")\n",
        "    paper_ids_list = list(paper_ids_set)\n",
        "    if len(paper_ids_list) > initial_pmid_count:\n",
        "        print(f\"Added {len(paper_ids_list) - initial_pmid_count} critical PMIDs. Total papers to process: {len(paper_ids_list)}\")\n",
        "\n",
        "    fetch_paper_titles.warned_no_approved_list = False\n",
        "    paper_info_dict, failed_title_ids = fetch_paper_titles(paper_ids_list)\n",
        "    if failed_title_ids:\n",
        "        print(f\"Retrying {len(failed_title_ids)} failed title IDs with batch size {RETRY_BATCH_SIZE}...\")\n",
        "        retry_info, _ = fetch_paper_titles(failed_title_ids, b_size=RETRY_BATCH_SIZE)\n",
        "        paper_info_dict.update(retry_info)\n",
        "    logging.info(f\"Retrieved titles, authors, and abstracts for {len(paper_info_dict)} papers.\")\n",
        "    print(f\"Fetched details for {len(paper_info_dict)} papers.\")\n",
        "\n",
        "    processed_paper_ids = list(paper_info_dict.keys())\n",
        "    citation_counts_dict, failed_citation_ids = fetch_citation_counts(processed_paper_ids)\n",
        "    if failed_citation_ids:\n",
        "        print(f\"Retrying {len(failed_citation_ids)} failed citation IDs...\")\n",
        "        retry_citations, _ = fetch_citation_counts(failed_citation_ids, b_size=RETRY_BATCH_SIZE)\n",
        "        citation_counts_dict.update(retry_citations)\n",
        "    logging.info(f\"Retrieved citation counts for {len(citation_counts_dict)} papers.\")\n",
        "\n",
        "    s2_citation_counts_dict = {}\n",
        "    if processed_paper_ids:\n",
        "        s2_citation_counts_dict = fetch_s2_citation_counts(processed_paper_ids)\n",
        "    logging.info(f\"Retrieved Semantic Scholar citation counts for {len(s2_citation_counts_dict)} papers.\")\n",
        "\n",
        "    paper_to_ncbi_genes, failed_gene_ids = fetch_linked_data(processed_paper_ids, \"gene\", \"pubmed_gene\", \"NCBI genes\")\n",
        "    paper_to_ncbi_variants, failed_variant_ids = fetch_linked_data(processed_paper_ids, \"snp\", \"pubmed_snp\", \"NCBI variants\")\n",
        "\n",
        "    all_ncbi_gene_ids = list(set(sum(paper_to_ncbi_genes.values(), [])))\n",
        "    print(f\"Total unique NCBI-linked gene IDs to fetch names for: {len(all_ncbi_gene_ids)}\")\n",
        "    gene_names_dict = {}\n",
        "    if all_ncbi_gene_ids:\n",
        "        gene_names_dict, _ = fetch_gene_names(all_ncbi_gene_ids)\n",
        "        logging.info(f\"Retrieved names for {len(gene_names_dict)} NCBI-linked genes.\")\n",
        "\n",
        "    all_ncbi_variant_ids = list(set(sum(paper_to_ncbi_variants.values(), [])))\n",
        "    print(f\"Total unique NCBI-linked variant IDs to fetch details for: {len(all_ncbi_variant_ids)}\")\n",
        "    variant_details_dict = {}\n",
        "    if all_ncbi_variant_ids:\n",
        "        variant_details_dict, _ = fetch_variant_details(all_ncbi_variant_ids)\n",
        "        logging.info(f\"Retrieved details for {len(variant_details_dict)} NCBI-linked variants.\")\n",
        "\n",
        "    results_list = []\n",
        "    print(\"\\nProcessing and structuring results...\")\n",
        "    for pmid, info_data in tqdm(paper_info_dict.items(), desc=\"Structuring results\"):\n",
        "        title = info_data.get(\"title\", \"N/A\")\n",
        "        year = info_data.get(\"year\", \"N/A\")\n",
        "        journal = info_data.get(\"journal\", \"N/A\")\n",
        "        has_target_author = info_data.get(\"has_target_author\", False)\n",
        "\n",
        "        text_genes = info_data.get(\"text_genes\", [])\n",
        "        text_variants = info_data.get(\"text_variants\", [])\n",
        "        text_variant_context = info_data.get(\"text_variant_context\", [])\n",
        "\n",
        "        linked_gene_ids_for_paper = paper_to_ncbi_genes.get(pmid, [])\n",
        "        ncbi_gene_symbols = [gene_names_dict.get(gid, f\"ID:{gid}\") for gid in linked_gene_ids_for_paper \\\n",
        "                             if gene_names_dict.get(gid) and gene_names_dict.get(gid) != \"Unknown Symbol\"]\n",
        "        ncbi_gene_symbols = sorted(list(set(s for s in ncbi_gene_symbols if s)))\n",
        "\n",
        "        linked_variant_ids_for_paper = paper_to_ncbi_variants.get(pmid, [])\n",
        "\n",
        "        pubmed_citation_count = citation_counts_dict.get(pmid, 0)\n",
        "        s2_citation_count = s2_citation_counts_dict.get(pmid, 0)\n",
        "\n",
        "\n",
        "        if (text_genes or text_variants or text_variant_context) and \\\n",
        "           not ncbi_gene_symbols and not linked_variant_ids_for_paper:\n",
        "            logging.info(f\"Paper {pmid} has text-derived entities but no NCBI-linked genes/variants.\")\n",
        "\n",
        "        processed_genes_for_this_paper = set()\n",
        "\n",
        "        for gene_symbol in ncbi_gene_symbols:\n",
        "            results_list.append({\n",
        "                'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': gene_symbol,\n",
        "                'Variant': '', 'VariantContext': 'NCBI_Linked_Gene', 'Alleles': '',\n",
        "                'Citations_PubMed': pubmed_citation_count,\n",
        "                'Citations_S2': s2_citation_count,\n",
        "                'AuthorFlag': has_target_author\n",
        "            })\n",
        "            processed_genes_for_this_paper.add(gene_symbol.upper())\n",
        "\n",
        "        for vid in linked_variant_ids_for_paper:\n",
        "            v_info = variant_details_dict.get(vid, {\"rsid\": f\"ID:{vid}\", \"alleles\": \"N/A\", \"gene\": \"\"})\n",
        "            variant_gene = v_info['gene']\n",
        "            if variant_gene and variant_gene != \"Unknown Symbol\":\n",
        "                results_list.append({\n",
        "                    'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': variant_gene,\n",
        "                    'Variant': v_info['rsid'], 'VariantContext': 'NCBI_Linked_Variant',\n",
        "                    'Alleles': v_info['alleles'],\n",
        "                    'Citations_PubMed': pubmed_citation_count,\n",
        "                    'Citations_S2': s2_citation_count,\n",
        "                    'AuthorFlag': has_target_author\n",
        "                })\n",
        "                processed_genes_for_this_paper.add(variant_gene.upper())\n",
        "            elif v_info['rsid'] != \"UnknownRSID\":\n",
        "                 results_list.append({\n",
        "                    'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': f\"FromVariant_{v_info['rsid']}\",\n",
        "                    'Variant': v_info['rsid'], 'VariantContext': 'NCBI_Linked_Variant_GeneUnknown',\n",
        "                    'Alleles': v_info['alleles'],\n",
        "                    'Citations_PubMed': pubmed_citation_count,\n",
        "                    'Citations_S2': s2_citation_count,\n",
        "                    'AuthorFlag': has_target_author\n",
        "                })\n",
        "\n",
        "        if text_genes:\n",
        "            for gene in text_genes:\n",
        "                if gene.upper() not in processed_genes_for_this_paper:\n",
        "                    best_variant_for_text_gene = text_variants[0] if text_variants else \"\"\n",
        "                    context_str = \"Text_Derived_Gene\"\n",
        "                    if best_variant_for_text_gene:\n",
        "                        context_str = \"Text_Gene_With_Text_Variant\"\n",
        "                    elif text_variant_context:\n",
        "                        context_str = f\"Text_Gene_Context:{text_variant_context[0]}\"\n",
        "\n",
        "                    results_list.append({\n",
        "                        'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': gene,\n",
        "                        'Variant': best_variant_for_text_gene,\n",
        "                        'VariantContext': context_str,\n",
        "                        'Alleles': '',\n",
        "                        'Citations_PubMed': pubmed_citation_count,\n",
        "                        'Citations_S2': s2_citation_count,\n",
        "                        'AuthorFlag': has_target_author\n",
        "                    })\n",
        "\n",
        "        paper_has_entry_in_this_iteration = False\n",
        "        if processed_genes_for_this_paper or linked_variant_ids_for_paper or (text_genes and any(g.upper() not in processed_genes_for_this_paper for g in text_genes)):\n",
        "             paper_has_entry_in_this_iteration = True\n",
        "\n",
        "\n",
        "        if not paper_has_entry_in_this_iteration:\n",
        "            placeholder_gene = \"NoGeneData\"\n",
        "            placeholder_context = \"NoGeneticInfoFound\"\n",
        "            if nlp and not text_genes and not text_variants and not text_variant_context:\n",
        "                placeholder_gene = \"SciSpaCyHit_Filtered\"\n",
        "                placeholder_context = \"SciSpaCyHit_NotValidatedOrLinked\"\n",
        "\n",
        "            results_list.append({\n",
        "                'PMID': pmid, 'Title': title, 'Year': year, 'Journal': journal, 'Gene': placeholder_gene,\n",
        "                'Variant': '', 'VariantContext': placeholder_context, 'Alleles': '',\n",
        "                'Citations_PubMed': pubmed_citation_count,\n",
        "                'Citations_S2': s2_citation_count,\n",
        "                'AuthorFlag': has_target_author\n",
        "            })\n",
        "            logging.info(f\"Paper {pmid} ('{title[:50]}...') added with placeholder: Gene='{placeholder_gene}', Context='{placeholder_context}'.\")\n",
        "\n",
        "    results_list.sort(key=lambda x: (x.get('Citations_S2', 0), x.get('Citations_PubMed', 0), x.get('PMID', '')), reverse=True)\n",
        "\n",
        "    if results_list:\n",
        "        fieldnames = ['PMID', 'Title', 'Year', 'Journal', 'Gene', 'Variant', 'VariantContext',\n",
        "                      'Alleles', 'Citations_PubMed', 'Citations_S2', 'AuthorFlag']\n",
        "        csv_file = f\"pubmed_genetic_results_{uuid.uuid4().hex[:8]}.csv\"\n",
        "        with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile_obj:\n",
        "            writer = csv.DictWriter(csvfile_obj, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(results_list)\n",
        "        print(f\"\\nResults saved to {csv_file}\")\n",
        "        logging.info(f\"Saved {len(results_list)} rows to {csv_file}\")\n",
        "    else:\n",
        "        print(\"\\nNo results to save.\")\n",
        "        logging.info(\"No results generated to save to CSV.\")\n",
        "\n",
        "    print(\"Script finished.\")\n",
        "    logging.info(\"Script finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P-ege8MOOO1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
